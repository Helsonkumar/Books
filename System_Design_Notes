System Design Notes:
==================================

Proxy Vs Reverse Proxy
---------------------------
Proxy : 
1. Anonymity (clients are hidden from the service provider
2. Caching
3. Geo fencing
4. Blocks unwanted content

Reverse Proxy :
-----------------
1. Load Balancing
2. Caching
3. Canary Deployment ==> Where a new change is deployed in one server and few of the client traffic is servered by this updated server.Otherwise called A/B testing.


Load Balancing:
-------------------
This is a special case of reverse proxy. 
Here the proxy server stores and manages the metadata of the backend server.
Eitheir distributes the load in ROUND_ROBIN fashion or based on the hearbeat or health check result of the backend server.

Alogrithms used by LB:
-----------------------------
1. Round Robin
2. LeastConnection : Server with less active connection and high resource availability and servers with high repsonse time
3. Hash : Where client IP address or requets URL is hashed and the client request is served with the specific server among the set of server.
4. Random pick of 2 : Pick any 2 servers randomly and use the least connection algorithm to derive the best server to serve clients request.


######################################################################################################################################################################################################################################################

Scaling Systems:
------------------------
Horizontal Scaling & Vertical Scaling

Horizontal Scaling is waht we use when we need have:

**High Throughput
**High Availability
**High Resource capacity

But Horizontal Scaling is actually limited by :
1. Wheather Ur app is of Shared-Nothing Architechtured or not.
2. Stateless.

In those cases horizontal scaling becomes more challenging.


######################################################################################################################################################################################################################################################

Load Balancer:
-------------------
1. Enhances user response time
2. Increases application uptime
3. Does the SSL offload like decrypting the SSL user request and off loading that task from the app servers.
4. Redundancy  : Does the failover switching task like when a server dies it handover the task to other server.
5. Flexibility : IN server maintanance. Since the traffic is routed to other serves.
6. Scalability : A new server can be added and the LB recognizes it as soon as it is added without downtime.

LB must do the hearbeat health check of the backend servers.


Application load balancer:
------------------------------
Developers code listeners which reacts to sepcific events and route the request to the specific target based on the content of the web request.

Hardware vs. software load balancing:

H/W LB:
--------
H/W LB direct traffic to servers based on criteria like the number of existing connections to a server, processor utilization, and server performance.
Hardware load balancers include proprietary firmware that requires maintenance. They come in the form of device with preinstalled or prebuilt LB S/W.
Less flexible


S/W LB:
--------
Highly flexible.
They come in the form of S/W installed in VM or installable S/W.
They also come in the form of LBaaS from cloud vendors off loading the maintanance.

LB Algorithms:
-------------------
Algorithms vary and take into account whether traffic is being routed on the network or the application layer.
Load balancing algorithms fall into two main categoriesâ€”weighted and non-weighted. 
Weighted algorithms use a calculation based on weight, or preference, to make the decision (e.g., servers with more weight receive more traffic).
Non-weighted algorithms make no such distinctions, instead of assuming that all servers have the same capacity.

LB Methods:
--------------------------------
Round Robin          :

Weighted Round Robin :   With this method, each server is assigned a weight (or preference), usually commensurate with its capacity.

Sticky Session       :   This method links specific clients and servers for the duration of a session.
                         Once the link is established, all requests from the user are sent to the same server until the session is over.
						 
Least Connections    :   The server handling the lowest number of requests receives the next request that comes in.

IP Hash              :    This algorithm creates a unique hash key based on both the source and destination IP address of the client and the server.
                          The key is used to route the request and enables a dropped connection to be reestablished with the same server.

LB on Cloud env is similar to the classical or general LB.

Clustering vs LB
---------------------
IN cluster ech server is aware of each other.
In LB each node is not aware of each other.

Open source LB:
------------------
Always check for the compatibility of the LB with its backend server.

LB can be done in Database level as well : Do check for thsi LB. DB LB can maintain the data integrity in a DB txn.


Layer 4 and Layer 7 LB:
-----------------------------------------------
Layer 7 : Application Layer:  HTTP Layer
http hearde , cookies, content-type
-----------------------------------------------
Pros : 
Smart Load balancing since it can route the request based on the data on the request header.
Good for M/S/W
Caching since it can look at data.
It can offload server side data encryption.


Cons:
Expensive(since it looks at data).
2 TCP connection.It looks at Ur data.
Less secure since data is present.



-----------------------------------------------
Layer 4 : Transport Layer : TCP Layer 
-----------------------------------------------
We know the ports and IP alone in this layer.
Does not have access to Data
Does the routing based on the IP address of the client.
One TCP connection.
NO encrytpion of decryption of the user data.

Pros:
Simple LB.Efficient . Since no data is looked up for routing.
More Secure. Since data is in the form of packets.
One TCP connection between destination and the source.


ALL LB happens via the NAT(Network ADdress Translator).

######################################################################################################################################################################################################################################################
CQRS  Pattern  : Command Query Responsibility Pattern :
----------------------------------------------------------
We seggregate the Read and Write Model. Since both of them would need diff representation of the data.
This would avoid the conjetion and data lock of the resources while data read and write.
Effective Scaling of each read and write models.
CQRS : Event sourcing :
UPdate records are dripped in the form of events in to the kafqka Queue and the read model play the events and update the state of the read Db using those events.



######################################################################################################################################################################################################################################################

Dist System Architecture: Sunu Sasidharan
----------------------------------------------------------
youtube.com/watch?v=6pjGuuGsqxE

Large Scale System(LSS)  is charactreized by  :
-------------------------------------------
Huge Data
Large Concurrent Users
Scalability Requirement
Throughput Requirement

These must be addressed in any LSS

Challenges in LSS:
------------------------
Role of Arch is multi faceted
Understand the Domain first to make a Arch decisionProd owner and stake onwer must be involved
They have to undertsand how integration must be done and how it can done iteratively

Compliance F/W : Landscape is fragemented
GDPR + HIPPAA + SOC + PCi DSS
They have overlap. This must be considered from the beginning

R/D for new tools and how they can be mainstreamed in Dev work


Dist System:
----------------
Why ?
To address the aboe challenges
Adv : Take adv of cloud native features

Challenges: Understand Domain

Way to do DS : M/S/WChallenges: Draw boundries
System must work cohesively together
So understand the Domain.
Undertsand CAP theorem

Styles for Implementing CAP theorem:
Eventual Consistency. High Availability ==> Use Kakfa for m/S interaction with messages.it can scale well
Strict Consistency . But low availability
Design M/S accordingly. Check where your business model si aligned.

Does the order of the message processing in queue matters ? It can scale well if order does not matters
fault tolerant : How resilient is your syste. 
Have U considered all possible cases.

Event Sourcing : U can have immutable system.
U can play back the message U arrive latest state.

Event Sourcing  + message queue  ==> Resilient System

Challenges: Eventual Consistency + Message Processing Order


Microservice Architechture:
================================
How to ensure M/S boundries.
it is not easy  : Keep improving the bounries.
Understand the DomainU can use Doamin Driven Design

Approach : Start with Monolithic System
Have /s within the monolith.
Once stable : Break the monolith into m/S
Have Seperation within the domain

Multiple team : Have the Lean team (Cross fucntional team : Lean methodologies)
each team design its won microservices
 Conways' law
 Dedicated team for each microservices
 Enhanced velocity of delivery
 
Contracts between the services must be defined quiet essesntially.

Monitor , Logging , Tracing

Containerization : Observability
Isynga , Pager duty

Test Driven Development:
TDD must be adopted with BDD .
Should be like an essential Cultural things

Processing Workloads :
=========================
CPU intensve  : Data transformation ,etc
IO intensive  : DB interaction , disk write,etc 

Node Js : event loop based proessing for low IO 
Avoids thread switching

Workload type of M/S must be considered.

Non-Functional Requirements
--------------------------------
Security + Performance  + Browser Compatibility + Accessibilty testing

Securtity  : Static Code analysis
Container with right patching + Cv tc
Vulnerabilittes scanner

Iac : Terraform , Ansible
helps patching system  : Blue Green deployment

Performance driven development

Concurrent testing

Observeability : Graffana dashboard ==> eahc M/S owner get a metric of their sercvice and take action accordingly

Distributed Log Tracing : LogStash or FluentD

FluentD collects the log ==> Stores in Elastic Search  ==> Kibana visualizes the monitoring data

In Nutshell : You must have 
**Dist System Design and fucntionality
**CAP Theorem
**Workloads
**Tooing
**Development methodologies

######################################################################################################################################################################################################################################################

Scaling a Db  :
Reason  : Too much load 

**Replicate the Data across multiple Instance
Master has all Writes and Slaves has all reads
Write cannot scala since master alone handles it
Replication lag due to delay in writing data


Reason  : Too much Data
Go for Sharding
Cons : Complex Queries like Range queries
       Joins become Difficult
	   
when to shard a Db :
------------------------
Before sharding we can do multiple things before that


Db Partitioning and sharding :
=============================
Sharding can be doen at Application level(redis and memcached) or at Db level (MongoDb, Hbase , Cassandra)


Partitioning Method:
------------------------
***Vertical Partitioning:
Based on some column (Hive partitioning)
It means splitting a large table into multiple smaller chucks of specific column value.


**Horizontal Partitioning:
Sharding
Splitting based on Rows ange on a key value.


But how the routing of request is done:
=========================================
Algorithmic Sharding : @ Application level using Consistent hashing algorithm : Memcached Db uses this to some extent

Dynamic Sharding  : A lookup service decides which DB the request must be routed to. But this introduces the Single Point of Failure

Data Partitioning Problem:
-----------------------------
Shard Allocation Imbalance :
Hot Key : A specific server with a key is frequrntly accessed makes it overloaded.
Data redistribution  : probelm while doing data redist while adding or removing servers
May require data denormalization  : Due to inefficient joins
transaction is not possible across shards.

Vitess.io : A database clustering system for horizontal scaling of MySQL
 technology used to make the routing decision 
 
 
	   

 


               







