Spark Notes :
=====================

Check what is zero-copy clone
Is predicate push down applicable in all cases.



Cluster Manager  :
--------------------
Yarn Vs Spark Standalone cluster
YARN allows you to dynamically share and centrally configure the same pool of cluster resources between all frameworks that run on YARN. 
You can throw your entire cluster at a MapReduce job, then use some of it on an Impala query and the rest on Spark application, without any changes in configuration.
We can even dynamically allocate or turn off the executors in the yarn dynamic mode

Spark standalone mode requires each application to run an executor on every node in the cluster, whereas with YARN, you choose the number of executors to use.
We will have to create Standalone Master and Stad.alone Worker services.
We cannot run any other workload in this deployment mode

Dataproc  :
---------------
gcloud dataproc jobs submit spark --cluster=helsondp --region=us-central1 --class=sql.CsvReader --jars=SparkOnGCP-1.0-SNAPSHOT.jar --properties=spark.executor.instances=3,spark.executor.cores=2





Spark uses Hadoop’s client libraries for HDFS and YARN.
We have Hadoop free binary version as well.
Spark runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS), and it should run on any platform that runs a supported version of Java.
Spark runs on Java 8/11, Scala 2.12, Python 2.7+/3.4+ and R 3.5+. 
For the Scala API, Spark 3.0.1 uses Scala 2.12. You will need to use a compatible Scala version (2.12.x).
Apache Spark uses Arrow as a data interchange format, The Arrow format is designed to enable fast computation.
Apache Arrow defines a language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. 


Cluster Mode  : Driver(with Sc) runs within the Application Master process which inturn runs in side the cluster-scoped
it is common to use cluster mode to minimize network latency between the drivers and the executors.


Client Mode   : Driver runs inside the spark-submit(Client to cluster) process and the application master simply exists to ask for resources from YARN.The input and output of the application is attached to the console.
Thus, this mode is especially suitable for applications that involve the REPL (e.g. Spark shell).
use this mode if your client is physically colocated to the worker node since the worker talks to master during app execution

– In YARN cluster mode, the Driver is run on YARN Application Master run on random Core node )
– IN YARN Client  Mode, the Driver is run on Master node itself. Bcz the the driver runs inside the yarn client process which is launched in Master node. 


In Cluster mode  : A YARN Client process is created and it periodically polls the APP Master.

When log aggretaion is truend on you eill see all logs for all executors and driver in the console with the below commnd: Else we will have the go the Data node and check for the logs with the app-id /var/log/hadoop-yarn/containers/[application id]/[container id]/stdout
yarn logs -applicationId <app ID>


Config Commands  :
----------------------
--jars : to have the dependcy jars distributed among the executors classpath.This is meant to be a dependcy jars for all task submitted to cluster.
--files  : Custome fiels to be used in executors (like log4J properties).This willl be uploaded from local Fs to HDFS and then loaded to executors

/bin/spark-submit \
  --class <main-class> \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key>=<value> \
  ... # other options
  --jars/ --files
  <application-jar> \
  [application-arguments]
  
  Config :
  sparkContext.setConf / --conf flags / spark-defaults.conf

Advanced Dependency Management
When using spark-submit, the application jar along with any jars included with the --jars option will be automatically transferred to the cluster and these jars would be copied or cached to the driver and workers involved..

File:// ==> means the drivers https file server (the executors pull the jar form this location )
hdfs://, https://, https:// ==> straightforward
local:/// ==> the file is expected to be present in all worker nodes(useful if large jars need to be transferred.Reduce network IO)
Note that JARs and files are copied to the working directory for each SparkContext on the executor nodes.
With YARN, cleanup is handled automatically for these jars after app execution.




SparkUI : Is hosted by Driver node(WHIHC MATY BE RANDOM NODE). So u must go to the driver node.
Spark History server  : Basically hosted in masternode :18080


Spark infact needs kerbros token to access various Cluster service to do the job. Even the Oozie workflow must be enabled to collect such token



Caught from High performance spark :
---------------------------------------------
Spark infact needs a storage system and clustre manager.

The cluster manager simply allocates resources and launches the executor according to the config setup and orchestrates them.

RDD  : Lazily evaluated  , Immutable , In-Memory

Lazy evaluation  : helps us collate or combine operation that does not require comm to driver and execute them in single pass thru of data.
we can chain together operations with narrow dependencies and let the
Spark evaluation engine do the work of consolidating them.

Fault Tolerant  : Spark would not privde incorrect result in the event of host machine failure.
Bcz each partiton has its own dependcy info to recalculate them if lost.

An RDD interface has 5 major properties  : List of partitons , Dependent partitions , fucntion for iteraor , partitioner , Prefered location (gives the lcoation info)

Sparks performance advantage over MapReduce is greatest in use cases involving
repeated computations.
In mapreduce the intermediate result is wirriten to disk for each pas thru of data.

Option for mem-mgmt :
----------------------
Inmemory deserialized : data is stored in the memory in the form of object. CPU efficinet. But not memory effective.
In-memory Serialized : Objects in RDD in the form of deserialized stream of bytes.
On Disk  : RDD partitons too lrage can be stored in disk.but can be more fault-tolerant for long sequences of
transformations


A spark action triggers a job.
A job has a stage defined for each txn with wide dependency
Each stage has set of tasks (assigned to Task Scheduler in each node)

Narrow and Wide dependency

Narrow  : A parent has only one child.
Wide  : A parent has many child

Dependencies are explianed from parent perspective.

Wide Dep : Due to shuffle
happens for K,V paired data types.
During every shuffle we talk with driver.
Txns with narrow dependency can be clubbed togther and doen within one data pass thru.
This would result in good performance  
The no of task depends on the number of partition involved in the child partition.
Shuffles stages limit the parallelization.results in Disk IO for shuffle files and data movement
Partiton with wide dependecny has costly in being recomputiing since most of its child partition must be recomputed again.

Coalesce : is used to change the number of partitions in an RDD .  It might decrease or increase the partition number.
its a Narrow one when we decrease the number, It does not triggeres shuffle.
it is wide when we increase the number of partition.
Increasing the parttion number triggeres the shuffle and this is a wide txn.
Note  : Shuffle is triggered for Non K.V paired data as well. Coalesce and Repartition is a special case.
U can set shuffle true oro flase for coalesce

Repartiton  : is an alternate to coalesce. It triggeres shuffle.

For partiton increase prefer repartition over coalesce.

RDD type information is very imp. 

Take care of GC. GC triggers more serialization time.Avoid creating more objects.

Tip : Try reusing the same object instaed of creating a new object for every records read like we did in the aggregate scala example.
Means we need to use mutable objects.
But best to avoid using mutable objects in spark since they might lead to serlaization error and provide inaccurate result.

Serialization in spark :
==========================
The compiled code  (Static part ) of the code is transfered to the driver and the worker. This does not happen during the run time.
But during the runtime the reference to objects (instance of class) is passed to the worker from teh driver.
This is what the serialization during the runtime. This can cause the tassk not serializable error during runtime.

References to static parts of the code. These point to things that were established permanently during compilation, and have already been shipped to the workers.
References to objects — instances of classes. These were created by the driver at runtime, hence the workers have no copy of them. These must be serialized and shipped to the workers.










Spark SQl  : Is similar to RDD excpt they have schema information . 
This Schema info is used for effective Storage layer(Tungsten layer) and Catalyst Optimizer.
The schema information, and the optimizations it enables, is one of the core differences
between Spark SQL and core Spark.

Txn on DF is More of relational Type and on DS it is of Functional Type.

DataFrames are Datasets
of a special Row object, which doesn’t provide any compile-time type checking.

HIve Context : SQlContext  : HiveContext has much better SQL parser fucntionality and enalbes access to Hive based UDFs.



For Spark SQL  : The fulcrum is the "DataSource API". So any format of data can be analyzed. 
So user must implement a plugin using this DataSource API to analyze a custom formatted data.

Unlike RDD, using DF would allow the Sprk Optimizer to analyze our code and data effectively which allows for a new class of optimizationss

RDD , DataFrame,  SQL

RDD is not used these days since it is cumbersome.

DF and SQL are Structured API.
But they limit what can be expresssed. 
But they make the code more Declarative and More space for optimization (here where the Catalyst Optimizer comes into the picture)


SQL : Catalyst optimizer
==============================
How catalyst work:
-------------------- 
Analysis Phase:                                           Optimization Phase:                     Physical Planning                              Code Generation  (Converst the Physical plan into specialized Java code )
====================                                     =======================                 =========================                    =========================
Unresolved Logical Plan  ===> Resolved Logical Plan ===>  Optimzed Logical Plan   ===> Physical plans ===> Cost based model ==> Selected Physical plan       ====> RDD  ====> Result


Physcial plan  : Cost based model  : Its does smoething like Join Selection
Cost based optimization  : Considers data distribution unlike rule based optimization
 But rite now we dont do consideration of diff rules for cost optimization.
 Instead we currently generate only one Physical plan and we stck to that.
 
*** Step 1: User code is abstracted as Tree despite of the language in whihc it is written (Abstract Structure TYree : AST)

*** Step 2  : Transformation  : Logical plan to physical plan (using strategies) Implemented using Scala partial function and Pattern matching.

Logical plan : Defines the computation on the data w/o defining how to conduct such computation.

Rule are applied  : Rules are something like doing only Row Projection and Column Pruning.

so these rules are applied in batches to batch of rows.

Looks like the data layer or storage layer statictics is very imprtatnt to decode the effective query plan.

But the storage layer statistics may change at any point of time and it is out of Spark's control

So Databricks brought in 2 solutions: Delta lake and AQE

Delta lake  : Gives ACID model on Spark data. It is an open source storage layer. Better optimization oppurtunities.




 SHUFFLE  or BROADCAST divides the query (Job) execution into stages.
 Intermediate results are materialized at the end of query stages. (Query stages points are the pipeline break point)
 Query stage point is optimal for runtime optimization.
 
 Unlike WE HAVE BEEN THOGUTH, intermediate results between stages are written to Disk, so the optimal size of the data is calculated for the AQE.
 
 ***So the query plan is re-calculated at the end of every stages.
 
 Effect of partition being too small and too large :
 ===================================================
 GC pressure, disk spill
 inefficient IO , Scheduler overhead , task setup
 
 Effect of Skew join or Skew data in general
 ================================================
 long running task for a stage among list of tasks.
 Disk spilling.
 
 
 
 
 
 Spark Memory Management  :
 ==============================
 How spark arbitrate memory between EXECUTIUON and STORAGE:
 ----------------------------------------------------------------
 
 Memory can be broadly classified into EXECUTION and STORAGE Memory
 Execution memory  : Used while performing some operation like grpBy , sort,etc
 Storage  : For caching the intermediate result
 
 Prior to Spark 1.6 :
 When Exec.mem exceed it usually gets spilled to disk and then read back to send the result back to User
 When Storage mem execceds then the LRU policy is applied to evict the blocks of data in storage memory
 
 In latest:
 -------------
 Both execution and storgae share the same memory space.
 When there is a memory contention from both execution and storage then the blocks in storage is evicted. Not the execution data is spilled to disk.
 
 But we also enforce some configs : which helps a peercentage of storage blocks  to retained when there is a memory contention.
 This is helpful for apps like ML whihc greatly relies on cached data for iterative computing.
 
 

 How spark arbitrate memory between tasks running in parallel:
 ----------------------------------------------------------------
 In Spark 1.0 the static assignemt  is used  : 1/N of the memory is assigned for each task statictly . N is the number of cores.
 Post Spark 1.0  : Dynamic assignemtn is effectiev  : Memory is dynamically assigned (in equal portion ) for each actively running task. If there is only one task then 1/1 (whole) memory si assigned to that single task.
 
 Static assignment: Is simple to implement . But does not handle straggler task whihc starves for memory. But dynamic assignement does.
 
 
How to arbitarte memory for operators running in a task :
---------------------------------------------------------
ds.grpBy.agg.sort
 
We have diff operators liek GrpBy + agg + sort
 
Whilst agg , the operator would use all the pages in memory to hold the HashMap is operates with.
So when all pages in the meory is occupied then Sort operator would not have any memory to work with.
So it fails.

So the idea is to share the meory pages mutually across all operaotr. But when the pages are exhaused then remaining operators would starve and fail.

So we use Coperative Spilling  : When there is a need for memory from other operators liek Sort, the agg  would spill it held data to disk and reassign that page to the sort operation co-operatively.

Summ :
-----------
So always avoid statictally assigning memory.
 
 
 
project Tungsten  :
======================
Unlike storing JVM object s(Which consumes more memory even for smal string like "abcd" .ie int are wrapped as BoxedIntegers and then stored) it stores the primitives in their native form so memory footprint is comparetively lesser.
Cache aware computation  : helps in lesser memory tracing
Off heap storage  : Off heap space can be used for storing cache. But usually heap storage(spark.executor.memory) is held untill JVM lifetime. when the JVMN is killed (Executor is killed then the cache data in the heap storage is erased.) 
But the items in off heap storage still remains effective.
Off heap is available since spark 2.0

WholestageCode generation  
============================
Final phase of query optimization. 
It’s a physical query optimization phase that
collapses the whole query into a single function, getting rid of virtual function calls
and employing CPU registers for intermediate data

The second-generation Tungsten
engine, introduced in Spark 2.0, uses this approach to generate compact RDD code
for final execution. This streamlined strategy significantly improves CPU efficiency
and performance.





****How to trouble Shoot OOM  :
---------------------------------------------
Case 1: Smaller partitions saved our job
-------------------------------------------
First, we check the basics: How much java heap do we allocate (using the parameter spark.executor.memory) and what is the share usable by our tasks (controlled by the parameter spark.memory.fraction)
Check for the memory consumption in Driver and Executor
ssh to respective nodes and do "top" command

Remote blocks and locality management in Spark
Thus, to avoid the OOM error, we should just size our heap so that the remote blocks can fit. ie the blocks whihc are present locally for executors.
Avoid allocating lareg remote blocks in memory  :  spark.maxRemoteBlockSizeFetchToMem.
Reduce the size of partitions


Case 2: Splitting the humongous objects
-------------------------------------------
The GC stealing our memory
We need a better understanding of G1 GC

Humongous fragmentation : 
G1 partitions its memory in small chunks called regions (4MB in our case).
When allocating an object larger than 50% of G1’s region size, the JVM switches from normal allocation to humongous allocation. This process requires contiguous free regions.
There is no process to gather free regions into a large contiguous free space. Even a full GC does not defragment

use parallel GC instead of G1

Storing 2-D arrays wor any arry DS would need contiguos memory  : So even when we have *Gb of free memory we were not able to allocate memory for an array of size 256MB which needs contiguos space worth of 256MB
So we tried splitting this large array into smaller one.


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
This chapter aims to answer the following questions:

Q1: After a successful deployment, what are the running services in each node?

Q2: How is the Spark application created and executed?

Each Worker manages one or multiple ExecutorBackend processes.
Each ExecutorBackend launches and manages an Executor instance.

In Standalone deployment mode, ExecutorBackend is instantiated as CoarseGrainedExecutorBackend.
We will have one CoarseGrainedExecutorBackend for one node for one app
Worker manages each CoarseGrainedExecutorBackend process thourgh an ExecutorRunner instance (Object).

Job Logical Plan:
--------------------
Def partition of child RDD  : max(partiton of parent RDDs)

In Spark, there are two kinds of data partition dependencies between RDDs:
NarrowDependency  : One child partiton FULLY depends on a small number of parent partition  (FULLY means the entire data in the parent maps to one child partition)
NarrowDependency essentially means each partition of the parent RDD is used by at most one partition of the child RDD
A dependency is a NarrowDependency, if the RDDs' #partition and partitioner type are the same.

Wide Dependency   : One child partiton depends on PARTIAL parent partition...



OneToOneDependency occurs if the partitioner type and number of partitions of the parent RDDs and CoGroupedRDD are the same. 
Otherwise, the dependency must be a ShuffleDependency. 

How does Spark keep multiple partition dependencies for each partition in CoGroupedRDD?

RDD_A.join(RDD_B) ==> CoGropuedRDD ==> MappedValuesRDD(Cartesian Product) ==> FlatMappedValuesRDD

If all the parent RDDs and CoGrpd RDD are HashPartitioned with same number of partitions then it is called HashJoin()
This would mean that there is One2One dependency betwen partitions

In summary, data partition dependencies are listed as below

NarrowDependency (black arrow)
RangeDependency => only used for UnionRDD
OneToOneDependency (1:1) => e.g., map(), filter()
NarrowDependency (N:1) => e.g., co-partitioned join()
NarrowDependency (N:N) => a rare case

ShuffleDependency (red arrow)


Job Physical Plan  :
-------------------------
In Narrow dependency   : No need to store any intermediate data. So the operations can be clubbed together. ==> g(f(record)
                         NarrowDependency chain can always be pipelined.
						 
In Shuffle Dependency  : Need to store intermediate data. So we cannot club the operations. 

Strategy for creating stages  :
------------------------------------
***Check backwards from the final RDD, add each NarrowDependency into the current stage
break out for a new stage when there's a ShuffleDependency. 

**In each stage, the task number is determined by the partition number of the last RDD in the stage.

Since the stages are determined backwards, the last stage's id is 0

 If a stage generates the final result, the tasks in this stage are of type ***ResultTask(reducer : if it gets data from parent stages else it is simply regarded as mapper)
 otherwise they are **ShuffleMapTask (mapper since its result needs o be shuffled to the next stage)


How Physical plan isexecuted  :
===================================
Recall in Haddop taska are executed in order

Map() ==> Shuffle+Sort+Aggregate(K,List<Values>) ==> Reduce

This execution process cannot be used directly on Spark's physical plan since Hadoop MapReduce's physical plan is simple and fixed, and without pipelining.

The computation chain is deduced backwards from the final step, but the actual execution streams the records forwards. 
One record goes through the whole computation chain before the computation of the next record starts.
For stages with parent stages, we need to execute its parent stages and then fetch the data through shuffle


Summary so far: The whole computation chain is created by checking backwards the data depency from the last RDD. 
                Each ShuffleDependency separates stages. 
				In each stage, each RDD's compute() method calls parentRDD.iterator() to receive the upstream record stream.(use RDD.getDependency() to get the parent RDDs)


DAGScheduler.runJob() ==> THis is called when an action is called on a RDD.
The tasks in a stage form a TaskSet. Finally taskScheduler.submitTasks(taskSet) is called to submit the whole task set.

Actor based  : DriverActor  + ExecutorActor


Coordination of system modules in job execution:
=====================================================
Master --> Driver --> Worker 
Each worker has CoarsegrainedExecBackedn  (Process) inside which the executor objects are scheduled and run. each executor has one CGEB. There might be many as er the number of Executor in a worker node.

Driver comprises of  (DAG scheduler  + taskScheduler  + SparkDeploySchedulerBackend(DriverActor : whihc receives task from taskschduler)) + mapoutputtasktracker(to handle result of shufflemaptask)
Every reducer has a BasicBlockFetcherIterator, and one BasicBlockFetcherIterator could, in theory, hold 48MB of fetchResults. As soon as one FileSegment in fetchResults is read off, some FileSegments will be fetched to fill that 48MB.

BlockManager is well designed, but it seems to manage too many things (data block, memory, disk and network communication)


cache and checkpoint:
==========================
Every time when a partition is expected to be computed(dependent rdd.iterator() in rdd.compute() method) it would be checked if the partition is already checkpointed or not.........


LinkedHashMap is waht used by Block Manager to hold the RDD partitions which are cached.
RDD are cached in blockmanager memory store or else checkpointed in DiskStore.

each partitons would have BlockID

Whn a block  (partition) needs to be computed, the block manager would be checked to see if the  partiton is cached/checkpointed.If cached local blockmager.getLocal(blockId) is called else blockMgr.getremote(blockId) is called.
Every blockmgr of the node would contact the blockManagerMasterActor  to indicate the location (blockMgrid) of the blocks being cached.
Using this location information, then the block is fetched from  the appropriate location.

Note, that caching the data frame does not guarantee, that it will remain in memory until you call it next time. Depending on the memory usage the cache can be discarded.
The cached partition can be evicted from emory and will be recomputed when needed.


Checkpoint:
===============
What kind of RDD needs checkpoint ?
the computation takes a long time
the computing chain is too long
depends too many RDDs

Checkpointing simply saves the state of some intermediate stages in a JOB.


 **************
 Checkpoints are recommended to use when a) working in an unstable environment to allow fast recovery from failures
 b) storing intermediate states of calculation when new entries of the RDD are dependent on the previous entries i.e. to avoid recalculating a long dependency chain in case of failure
 **************
 
Actually, saving the output of ShuffleMapTask on local disk is also checkpoint, but it is just for data output of partition.

Every time  an RDD needs to be cahced, the computed partition is immediately cached in memory.
But for checkpointing, spark wiats till the job compltes, and then another job is launched to recompute the RDD again then checkpint in DIsk,
Checkpointing and RDD persist only DISK_ONLY are both different.

User should set the storage path for check point (on hdfs).
RDDCheckpointData which manages checkpointed RDD

the difference between cache and checkpoint ?
Lineage information is maintained by the Blocks whihc are cached in memory or persisted in Disk(Local Disk) and they all are managed by block manager.
So they are recomputed even when there is a node failure using the lineage information.
But once the driver pgm finishes, then bloackMgr also would be killed and then all athe locally persisted data blocks would be lost.

But in checkpointing the lineage information is trucntaed and the data blocks are persisted in HDFS which is naturally fault toloerant 
and the bloks in HDFS remain intact even after the end of blockmanager.
So that the checkpointed RDD can be used by the next driver program.

 sc.setCheckpointDir("hdfs ://dev:/Users/xulijie/Documents/data/checkpoint")
  
  Very Importatnt  :
  =======================
  We check for a partiton being computed is already checkpointed or not. If yes, pick the check point
 runJob() will call finalRDD.partitions() to determine how many tasks there will be.
 rdd.partitions() checks if the RDD has been checkpointed via RDDCheckpointData which manages checkpointed RDD.
 If yes, return the partitions of the RDD (Array[Partition]).
 When rdd.iterator() is called to compute RDD's partition, computeOrReadCheckpoint(split: Partition) is also called to check if the RDD is checkpointed. 
 If yes, the parent RDD's iterator(), a.k.a CheckpointRDD.iterator() will be called. 
 
 CheckpointRDD reads files on file system to produce RDD partition. That's why a parent CheckpointRDD is added to checkpointed rdd trickly.
 
 
Summary  :
============
When Hadoop MapReduce executes a job, it keeps persisting data (writing to HDFS) at the end of every task and every job. 
When executing a task, it keeps swapping between memory and disk, back and forth. 
The problem of Hadoop is that task needs to be re-executed if any error occurs, 
e.g. shuffle stopped by errors will have only half of the data persisted on disk, and then the persisted data will be recomputed for the next run of shuffle. But in spark we will recompute the data only for the failed task.
Spark's advantage is that, when error occurs, the next run will read data from checkpoint, but the downside is that checkpoint needs to execute the job twice.








$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$








Spark Arch Shuffle  :
=========================
https://0x0fff.com/spark-architecture-shuffle/
https://github.com/JerryLead/SparkInternals/blob/master/markdown/english/4-shuffleDetails.md


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
when shuffling is triggered on Spark?????? ;;; Imagine Map + Reduce like processing
 Any join, cogroup, or ByKey operation involves holding objects in hashmaps or in-memory buffers to group or sort.
 
 distinct creates a shuffle
 
 reduceByKey and aggregateByKey

**Shuffle is not triggered if both DF/RDD in join holds the same partitioner 


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$


Mapper  (task that emits data ) -->. Shffle(happens between these two tasks) --> Reducer  (Consumes data)

spark.shuffle.compress + spark.shuffle.compress.spill  ==> Uses snappy codec to compress

 Each reducer should also maintain a network buffer to fetch map outputs. Size of this buffer is specified through the parameter spark.reducer.maxMbInFlight (by default, it is 48MB).
 
Diff shuffle impl are there  : Spark.shuffle.manager  : Hash / sort(default) / tungsten-sort

spark.shuffle.spill == true/false  : Decides if we need to spill the data to disk .
ie. Output of mapper if not fitted into the memory allocated for reducer task then this config would take effect. Def  : true (it wold spill data to disk if it does not fit memory)

The amount of memory that can be used for storing “map” outputs before spilling them to disk ==> JVM Heap Size” * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction

Spark internally uses AppendOnlyMap structure to store the “map” output data in memory. 

Hash Shuffle :
----------------
This use to generate R number of files corr to R reducer by each M mapper tasks..so M * R files are dropped by mappers in spark.local.dir
mapper output is not sorted.

sorting in Spark on reduce side is done using TimSort, 

 if you don’t have enough memory to store the whole “map” output? You might need to spill intermediate data to the disk. Parameter spark.shuffle.spill is responsible for enabling/disabling spilling, and by default spilling is enabled. 


Avoid groupByKey and use reduceByKey or combineByKey instead.

groupByKey shuffles all the data, which is slow.

reduceByKey shuffles only the results of sub-aggregations in each partition of the data.


Shuffle Process:
======================
Shuffle Comparison between Hadoop and Spark
Both has Mapper and Reducers.
Hadoop workflow: map(), spill, merge, shuffle, sort and reduce()

mappers simply partitons their output for corresponding reducer.
The reducer buffers this data in memory and aggregates the data.
In hadoop the output from Mapper is expected to be sorted before aggregation and applying reduce() logic ontop of it. 
But in spark we need to obviously mention it using sortByKey()(Uses external algorithm)
else use spark.shuffle.manager=sort

Shuffle Write  : At mapper (ShuffleMapTask)side , Process of partitioning the output for corresponding reducer is called Shuffle Write
Shuffle Read   : At reducer (ResultTask)side  , processe of  reading and aggregating data


Shuffle Write:
------------------
Each Mapper(ShuffleMaptask) would write a Shuffle Block File(File Segmenet).
Each ShuffleBlockFile corresponds to a Reducer.
For each SuffleBlockFile we have a ShuffleBuffer(Shuffle Bucket : Denotes the data for specific Reducer(whihc may corresponds to one or more than one Key)
Each Core would have corresponding Buckets : ShuffleMaptask woudl be executed in sequqnce in a Core for a stage. 
So C(cores) * R (Reducers) * M (Mapper task) ==> number of shuffleBlockFisles 
These shuffleBlock file would be persisteed in the disk at the mapper side. Whihc would be read by reducer later.
 But in later version of spark  : enabling spark.shuffle.consolidateFiles = true would allocated a shared file for each buckets(into whihc the shuffleblocks would be appended by the respective mapper tasks)
 So we will have one file per reducer for each core (C * R * 32KB ==> This much memory would be utilized at each node for this buffer allocation)
 The buffer size would be 32KB


Shuffle Read:
----------------------
DataBlocks --> parallelCollRDD -> MapPartitionRDD --> ;;;;; --> shuffleRDD --> mapPartitionRDD --> Result

The ShuffleBlocks are fetched after the completion of all ShuffleMapTask.

Once they are processed... Spark utilizes data structures like HashMap to do the job. Each <Key, Value> pair from the shuffle process is inserted into a HashMap. 
If the Key is already present, then the pair is aggregated by func(hashMap.get(Key), Value).

The fetched FileSegments get buffered in softBuffer

spark.shuffle.spill is false, then the write location is only memory.
A special data structure, AppendOnlyMap, is used to hold these processed data in memory.

MapR would expect the data to be sorted before aggregation. But in spark it not mandated. So spark is faster here
Otherwise, the processed data will be written to memory and disk, using ExternalAppendOnlyMap

How do the tasks of the next stage know the location of the fetched data? 
ShuffleMapStage, which will register its final RDD by calling MapOutputTrackerMaster.registerShuffle(shuffleId, rdd.partitions.size).
So during the shuffle process, reducers get the data location by querying MapOutputTrackerMaster in the driver process. 
ShuffleMapTask finishes, it will report the location of its FileSegment to MapOutputTrackerMaster.


Shuffle Read of Typical Transformations:
---------------------------------------------
Apply map side combine to reduce the network traffic.



HashMap in Shuffle Read:
----------------------------
Hash map is a frequently used data structure in Spark's shuffle process
Spark has 2 versions of specialized hash map: in memory AppendOnlyMap and memory-disk hybrid ExternalAppendOnlyMap.


AppendOnlyMap or ExternalAppendOnkyMap  : spark.shuffle.spill == true/false
-----------------------------------------------------------------------------------------------
Hadoop allocates 70% of the memory space of a reducer for shuffle-sort.
25% memory is allocated for shuffle in Spark
Spark has spark.shuffle.memoryFraction * spark.shuffle.safetyFraction (defaults to 0.3 * 0.8) for ExternalAppendOnlyMap
Each Reducer would hold these maps.
An executor holds a ShuffleMemoryMap: HashMap[threadId, occupiedMemory] to monitor memory usage of all ExternalAppendOnlyMaps in each reducer. 


spark.shuffle.spill == false 
AppendOnlyMap : Would be used when the shuffleData + processed data are to be held only in memory.
We have static allocated percentatge of ShuffleMemory : When that memory is reached then it would throw OOM

ExtrenalAppendOnly  : spark.shuffle.spill == true
If the appendonly map is full then check for available memory .
If there's still enough space, the AppendOnlyMap doubles its size
otherwise **ALL** its key-value pairs are sorted and then spilled onto local disk (by using destructiveSortedIterator()). 
 In each spill, a spillMap file will be generated and a new, empty AppendOnlyMap will be instantiated to receive incoming key-value pairs.
 when asked for the final result, a global merge-aggregate needs to be performed on all spilled maps and the in memory

Global merge-aggregate runs as follows.

Spill process. Like the shuffle write, Spark creates a buffer when spilling records to disk. Its size is spark.shuffle.file.buffer.kb defaulting to 32KB.
Spark limits the records number that can be spilled at the same time to spark.shuffle.spill.batchSize, with a default value of 10000.






















Optimizing and Tuning Spark Applications :
================================================
Performance optimization lessons from Spark+AI and Data+AI Summits

Sizing partitions:
---------------------
**spark.sql.shuffle.partitions. : Configures the NUMBER(not size) of partitions to use when shuffling data for joins or aggregations.
This defines the number of partitons of the resulting DF wheich is the outcome of any  or AGG or GRP_By operation.
Def  : 200 
This is an issue for  small sized data  : Since we will end up with more partitions.
For large data : We will have to stick to 200 and we will not use the resources effectively
How to dynamically set this  : sparkSession.conf.set("spark.sql.shuffle.partitions",100)
The exact logic for coming up with number of shuffle partitions depends on actual analysis. You can typically set it to be 1.5 or 2 times of the initial partitions.

** the NUMBER OF SHUFFLE PARTITIONS = INP DATA SIZE / TARGET SHUFFLE PARTITION SIZE

Lets say inp : is 54GB and target 100MB : 54GB/100MB = 540 partitions
But we need to check for the available cores and see how equally we can distribute the partitons across all cores.
Lets say we have 96 cores : then 540 /96 = 5.625 partitions per core. so the first 5 partitions for every core would be processed in the first cycle.
In the secodn cycle the remaining 0.625 partitons would be processed, this would lead to 35% of the cores to stay idle.
So we need to chahnge that 540 to 480 so each core would get 5 partitions instead of 5.625...No need to second cycle here.

Bad shuffle partitions size can increase spills to the disk and also under optimize resources 

** spark.sql.files.maxPartitionBytes  : inp file's partition size. 
def  : 128MB


Dont shuffle multiple times in code...like df.repartition().grp_by()

Unpersist the cached object when not needed.
NOte  : Mem+ Disk level persist would not evict the LRU object from emory instead it would write data to disk if memory is full which is inefficient.

use NOT EXISTS instead of NOT IN operator






**spark.default.parallelism  : effective for RDD only and not for SQL DF. 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 



AQE  : Adaptive Query Execution in Spark 3.0
=================================================
It simply optimizez the layer between logical and Physical plan dynamically by using the runtime metrics of the stages involved in the spark pipeline.

At present, the following optimizations have been implemented in version 3.0:

****Dynamically coalescing shuffle partitions : Earlier it use to be a static one. But now it can be dynamic.
****Dynamically switching join strategies
****Dynamically optimizing skew joins

Dynamically coalescing shuffle partitions
------------------------------------------
Transformations on a dataset deployed on Spark can be of two types: narrow or wide

Wide-type data needs partition data to be redistributed differently between executors to be completed. 
e.g groupBy operation . Joins ,etc

The number of partitions after optimization will depend instead on the setting of the following configuration options:
spark.sql.adaptive.coalescePartitions.initialPartitionNum
spark.sql.adaptive.coalescePartitions.minPartitionNum
spark.sql.adaptive.advisoryPartitionSizeInBytes

where the first represents the starting number of partitions (default: spark.sql.shuffle.partitions), 
the second represents the minimum number of partitions after optimization (default: spark.default.parallelism), and 
the third represents the “suggested” size of the partitions after optimization (default: 64 Mb).


Joins  :
=====================
Joins in spark is expensive due to shuffles files.
if the datasets do not have known partitioner(partitioned with same partitioner) then we may have data (value for the same key) unevenly distributed acros nodes.
Ideal spark tends to collect data or value for a key present in the same partition or in the same node.
So the same key from 2 diff datasets would be made to present in the same node so the data for that key can be joined locally. This triggers the shuffling of files across network.
But if the datasets are already partitioned by the same partitioner then the values would present in the same node.Shuffle would not be triggered in this case.
Spark in geberal does the qui-join.ie.join only the keys whihc are rpresent in 2 DS.
Supports right-outer,left-outer,full-outer,left-semi,left-anti
Default  : Shuffle Hash join

Joins in RDD is not as optimized as Join in Datasets or Dataframes.
In Spark-sql the filetr push down and BC hash join ia all taken care by the optimizer.

Broadcast Hash join :
-------------------------
To avoid large shuffle, the smallest DS from a join would be completly broadcasted to the nodes related to the larger node.
This reduces network IO.

Static Partiton pruning :
-----------------------------
Used to consider only the partitons whihc are related to the join while scan.
if we use any of the partiton column for fileter then static pruning is triggered.

 
 
 

Dynamic Partition pruning  :
=================================

The mechnism for Spark to prune scanning unnecessary partitons in a query
This an optimization from Spark v3.0

Dynamic partition pruning occurs when the optimizer is unable to identify at parse time the partitions it has to eliminate. 

*** This optimization is triggerred only when your join query is planned as BROADCAST-HASH JOIN.
*** when one table in the join is below 10MB(treshold to consider a table as small to be used for Broadcasting)

This optimization occurs at the Logical Plan level.

This is useful for running effective  STAR SCHEMA JOIN queries : Queries which involve the big fact tables and multiple Small dimensional tables.

Lets say we join sales fact table with the date dim tale. And we have filter for Dim table alone .
Since the Dim table is small in size here , spark would do the broadcast join .ie
The Filtered result from the Dim table is broadcasted to the nodes where the partiton for the fact table resides.
This would help us avaoid shuffles.

DPP uses this approaches. 
The filtered applied on DIM tables is plugged into the scanner of the larger Fact tables, so only the fact partitions which are actually needed for joining would be scanned.
This is what DPP all about. 
We avoid scanning unnecessary scanning in Star-Schema join queries.

pre-requisite :
-------------------------
This would work only if there is a matching partition column between fact and dimension tables.
The size of the dim table must be under 10MB configured level. To be considered as Broadcast Hash join.
DPP is not triggered for other type of joins.
Because the dynamic filtering is a kind of subquery duplication. (subquery holds the filetered result of the dim table)
So as long as the Dim table subquery duplicate execution is cheaper, it is ok.
if the Subquery duplication is costly then spark would not triggere this DPP.




Imaging that dim has column date which is not a partition column in the fact table.
Then the entire fact tables would be scanned anyway , and the DPP is not applicable in this case.

This can be said as a challenge faced.

















Spark On Kubernetes: as of spark 3.0.1 version
=========================================================

To make the most of Apache Spark, we need two things:
A distributed storage to store the data
A scheduler to run the Spark executors across a computing cluster
People have been doing this differently on-premise and cloud. With on-premise, most use Spark with Hadoop, or particularly HDFS for the storage and YARN for the scheduler. While in the cloud, most use object storage like Amazon S3 for the storage, and a separate cloud-native service such as Amazon EMR or Databricks for the scheduler. 


Reason  :
-------------

In general Seperation of Storage and Computation is what needed. This is somewhat addressed by containerized apps in kubernetes.

The primitives offered by k8s like namepsaces, pods, deployment, raset,etc has been reason for running spark on k8s instead of yarn.

***Simpler Administration
    Kubernetes allows for the unification of infrastructure management under a single type of cluster for multiple workload types. You can run Spark, of course, 
	but you can also run Python or R code, notebooks ,and even webapps. 
	In the traditional Spark-on-YARN world, you need to have a dedicated Hadoop cluster for your Spark processing and something else for Python, R, etc.

Isolation, meaning that you can prevent workloads running in the cluster from interfering with each other because they will remain in their own "namespaces.” 


*** Better resource management,
    the scheduler takes care of picking which node(s) to deploy the workloads on in combination with the fact that in the cloud, scaling a cluster up/down is quick and easy 


***Easier Dependency Management across Workloads
   Managing dependencies in Hadoop is time-consuming: packages have to exist on all nodes in a cluster, 
   makes isolation (i.e., making different versions of the same software - like TensorFlow 1.5 and TensorFlow 2.0 - coexist on the same node) difficult and updating environments challenging.
   the hassle of maintaining a common dependency for all workloads running on a common infrastructure is removed due to docker containers with their onw dependencies.
   
   
***More Flexible Deployment
   If you’re doing Enterprise AI and want to start moving data storage to the cloud (or have already started doing so), the idea of vendor lock-in can be very scary. 
   That’s why today, more and more businesses are taking a cloud-agnostic approach.
   Running Spark on Kubernetes means building once and deploying anywhere, which makes a cloud-agnostic approach scalable.
   We dont have to refactor our CI/CD pipelines for each env like EMR, DataProc , HDInsight
   
   
*** It's Cheaper
       
	   
	   
Pitfalls:
-----------
Spark-submit is added with support to Kubernetes since v2.3. Supports both cluster and client(since 2.4) mode
But Kubernetes operator provides a lot more feature owing Monitoring and management.
Although easy to use, spark-submit lacks functionalities like supporting basic operations for job management. 
 Thus, users will have to manage their jobs via the Kubernetes tools like kubectl in this case.
 
The Spark driver pod uses a Kubernetes service account to access the Kubernetes API server to create and watch executor pods.
By default, the driver pod is automatically assigned the default service account in the namespace specified by spark.kubernetes.namespace
if no service account is specified when the pod gets created else it uses : spark.kubernetes.authenticate.driver.serviceAccountName=<service account name>.

$ kubectl create serviceaccount spark
To grant a service account a Role or ClusterRole, a RoleBinding or ClusterRoleBinding is needed. 

Note that a Role can only be used to grant access to resources (like pods) within a single namespace,
whereas a ClusterRole can be used to grant access to cluster-scoped resources (like nodes) as well as namespaced resources (like pods) across all namespaces.

 
spark-submit vs operators:
-------------------------------
Client mode :spark-submit directly runs the driver at the spark-submit submission client side or within Pod and the executors run as pods in the kubernetes cluster
Cluster-mode : spark-submit delegates the job submission to kuberntes spark backend service whihc runs the driver in a pod.

We dont need spark pre-installed in the k8s nodes, since the spark version is decided by the spark-submit we use and the requireed spark core version is actually bundled on fly in the respective containers.
**So we can run app-spark2.4 and app-spark2.5 version simultaneously without any issues in the same k8s cluster in the same namepsace as well.



Dependency management in k8s :
------------------------------------
The application dependencies: Can be done in 2 ways
    They can be bundled into the container image itself (like uber jar)
	They can be passed as SPARK_EXTRA_CLASSPATH var inside the dockerfile used to build the application image or via local:/// . These app dependencies would be downloaded and added to driver pods classpath during run time.
	
	


The driver then interacts with K8s api-server for creation and management of the executors pods.
In spark submit everyting is driven by configuration properties (--conf)

spark-submit can be directly used to submit a Spark application to a Kubernetes cluster. The submission mechanism works as follows:
It comminucates with the k8s cluster thru a K8 clinet interface which can be customized


Spark creates a Spark driver running within a Kubernetes pod.
The driver creates executors which are also running within Kubernetes pods and connects to them, and executes application code.
When the application completes, the executor pods terminate and are cleaned up, but the driver pod persists logs and remains in “completed” state in the Kubernetes API until it’s eventually garbage collected or manually cleaned up.

POD Template :
----------------
Spark can be provided with config for Driver and Executor pods using pod template.

spark.kubernetes.driver.podTemplateFile 
spark.kubernetes.executor.podTemplateFile

the pod templates are used for the customization of driver and executor pods.
A spark pod can contain spec for more than one container.
 To allow the driver pod access the executor pod template file, the file will be automatically mounted onto a volume in the driver pod when it’s created.


Using Kubernetes Volumes:
----------------------------
Spark on Kubernetes in general interacts with Object Storage like GCS or AWS S3 or any other form of NFS for data storage read and writes.
The HDFS integration is still experimental.
So the IO is not effective here.
Also spark uses some scrtach psace to store the tmpFS for spill files.
Using DockerFS is slower.///So we use Volumes to hold these temp files


Starting with Spark 2.4.0, users can mount the following types of Kubernetes volumes into the driver and executor pods:
***hostPath: mounts a file or directory from the host node’s filesystem into a pod.(like host SSD)
***emptyDir: an initially empty volume created when a pod is assigned to a node. (This is part of nodes volume(temp volume)
             By default emptydir is created for each pod.
			 emptyDir volumes use the nodes backing storage for ephemeral storage by default,
			 
***persistentVolumeClaim: used to mount a PersistentVolume into a pod.
***RAM : Even RAM can be used for any diskless nodes.. But this is dangerous of loosing spill data.
         This part is alos considered while allocating memory for pods.
		 
--conf spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.path=<mount path>



Local Storage  :
---------------
spark.local.dir  : Used for data spills during shuffles.


Using RAM for local storage:
-------------------------------
For example if you have diskless nodes with remote storage mounted over a network, having lots of executors doing IO to this remote storage may actually degrade performance.
In this case it may be desirable to set spark.kubernetes.local.dirs.tmpfs=true in your configuration which will cause the emptyDir volumes to be configured as tmpfs i.e. RAM backed volumes.


Introspection and Debugging:
---------------------------------
Accessing Logs : kubectl -n=<namespace> logs -f <driver-pod-name>
                 or used Kubernetes Dashboard


Accessing Driver UI : $ kubectl port-forward <driver-pod-name> 4040:4040
Then, the Spark driver UI can be accessed on http://localhost:4040.


Debugging :  often, the best way to investigate may be through the Kubernetes CLI.
$ kubectl describe pod <spark-driver-pod>
$ kubectl logs <spark-driver-pod>

***Finally, deleting the driver pod will clean up the entire spark application, including all executors, associated service, etc. 
***The driver pod can be thought of as the Kubernetes representation of the Spark application.


Kubernetes Features
============================
Configuration File :
---------------------


Context  :
----------- 
Used to switch between multiple k8s cluster



Namespaces :
--------------
Namespaces are ways to divide cluster resources between multiple users (via resource quota).
Kubernetes allows using ResourceQuota to set limits on resources, number of objects, etc on individual namespaces. 
 
 





Spark Application Management:
=================================
Users can kill a job by providing the submission ID that is printed when submitting their job. 
 The submission ID follows the format namespace:driver-pod-name.
 
 $ spark-submit --kill spark:spark-pi-1547948636094-driver --master k8s://https://192.168.2.8:8443
 $ spark-submit --status spark:spark-pi-1547948636094-driver --master  k8s://https://192.168.2.8:8443



Future Work:
------------------
There are several Spark on Kubernetes features that are currently being worked on or planned to be worked on. Those features are expected to eventually make it into future versions of the spark-kubernetes integration.

Some of these include:

Dynamic Resource Allocation and External Shuffle Service : The shuffles files are lost whne the excutor node is scaled down in Dynamic allocation.
This is prevented by using the external shuffle service to external files systems.This saves even the scratch data which spark writes locally.


Scaling can be at App level or even at cluster level in K8s.
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.shuffleTracking.enabled=true

but in Dynamic Scaling the node start may take some time.
To kick start the pods immediately, you can oversize your Kubernetes cluster by scheduling what is called “pause pods” on it. 
These are low-priority pods which basically do nothing.
When a Spark app requires space to run, Kubernetes will delete these lower priority pods, and then reschedule them 
This prevent starting a new node and consumes no resource for pause pods.


Use on Demand machines for Drivers and Spot nodes for executors.



Job Queues and Resource Management



Spark Opeartors:
---------------------
based on operator pattern.
Which encapsulates the domain knowledge of running spark app with custom resources and the defines controllers for those resources.

The Operator defines two Custom Resource Definitions (CRDs), SparkApplication and ScheduledSparkApplication.
The later is for submitting spark jobs as per a timly schedule like schdule cron job
the Operator also gives you the ability to mount volumes and ConfigMaps to customize your pods.
In Operator everytiing is defined in the form of yaml file and have them deployed.
So that the spark driver and its relevant objects can be managed as a normal kube objects. 

So we need to have the Spark Opeartor installed in the kubernetes cluster first.
So it creates a custom controllers.
The job definition in the yml file defines the spark job object.
Once the yml file is submitted the operator calls the custom controller to create a Spark job object from the given job def in yml file
And then the componenets within the operator submits the spark-submit command.
The pods events are monitored by the component within the Opeartor pod.


What Is A Mutating Admission Webhook?
The webhook supports mounting ConfigMaps in Spark pods, which can come in handy in the following scenarios:















