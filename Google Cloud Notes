Building Scalable Systems:
=========================================================
All systems are eventually consistent.

Performance has limit
Scalability has no limits.

Eventul consistency : If no updates then all part of the system would be eventually consistent.
Types: Consual Consistent , Strong Consistent , Sequential Consistent

All systems are eventual consistent in natre . Think of GitHub code locks.

Using locks would help us achieve Strong Consistency. Think of Single Db accessed by amny parts of the system.
But lock adds more overhead like contention.
Strogn consistency is all about diff parts of the system responding with the same answer if ased at any time.
All parts must agree upon the state and respond with answer like group of Jury.
Strong consistency mean we dont wnat to have STALE DATA.

Contention :  Diff parts of system contending for a shared resource like DB record,this would lead toa state where more the load is , higher the time taken to release the resource.


Consistency and Availability cannot co-exist.

Law of Scalabilty:
=====================
Amdahl's Law : Contention Limits parallelization. Depicts the relation between Concurrenty and Throughput of the system.

Defines the limits of parallelization.
Piece of code we can do nothing about is the one whihc holds contention and which cannot be parallelized.


Coherency Delay :
======================
Think of meeting scheduling scenarios.

**** Concurrency Vs Throughput ****

Gunther's Law of Coherency Delay :
-----------------------------------
Accounts for Contention + Coherency Delay.

State of Coherence : Can be achieved by having members of the system agreeing upon the state change of each other. 
                     But this would result in a delay.This is called Coherency delay.
					 
Gunther's Law states that increasing concurrency can result in negative result due to Coherency Delay + Contention.

Linear Scalability is almost not achievable : Considering the communication and Contention and Coherence verhead between the parts of the system.

Stateless : Cannot be achived. Sharing state between parts of system losses the ability of Linear Scalability.


How Reactive System / Microservice enhances Scalability :
===========================================================
Avoids Contention by :
    Isolating Locks,Elimates Txn , Avoids Blocking operation.
	
Avoids Coherency-Delay by :
    Embracing eventual consistency 
	Building Autonomy ==> Systems which do not have to interact
	
This simply reduces them and not avoids them.


CAP Theorem:
===============================================================
Consistency Availability Partition Tolerance

CA side is not a valid in kost cases....either CP or AP is possible..

Partition tolerance : When part of the system goes  down and unable to commnicate with each other the system still remains respnsive.
Partition does not mean the data partition. It means the node or system unvailability scenario.

Dealing with Partition : Making the system to remain responsive even under the case of drop of n/w comm
============================================================================================================
1. CP : Dropping the system which has gone down. So we write to one node and have that for read as well. Here the consistency is achived but if that single node goes down then we loss availability.
2. AP : Start writing on both the nodes. Here availability is achived , but consistency of data is achived since the data on both nodes would remain inconsistent.We will have to merge data after partition is resolved.

In reality, most systems balance the Consistency & Availability concerns usually favoring one side or the
other.

By doing all reads and writes through a single master node, we can guarantee: Consistency
When we failover to a replica, we are sacrificing: Consistency

By allowing reads and writes to go to any single database node, this system is favouring: Availability
Think of writig to all replicas before txn completes.... : Here we compromise availability...


Contention Vs Scalability:
============================
Consistency creates contention.
Contention will result in demnishing result in Scalability

Isolating  Contention :
============================
In Reactive Apps Shardign at APP level ;; Not the Db level
Record level locking instead of table level locking.

To provide scalability, while maintaining Consistency we look to: Isolate resource of contention

Sharding for Consistency : Provides Strong consistency
----------------------------------------------------------
Nodes has many Shards  ==> A Shard consist of many Entities.


A shard cannot exist in more than one node.
An entity cannot locate in more than one shard.

Query for an entitiy is decided by entity ID.
We use HashCode algo on entity ID to decide the shard to which that entity belongs.
Choosing correct enity ID is very important.

Sharding at the application level provides which of the following benefits:
----------------------------------------------------------------------------
Here even if the Db does not supports sharding we can still do sharding at app level.
Reduced comm between App and DB.
Strong Consistency
Improved Scalability.

We can describe Sharding by saying that: Sharding partitions(Nodes) entities or actors in the domain according to their unique Id. correct

Sharding Co-ordinator will ensure proper routing of the Entity request.

Thumb Rule is to get 10 Shards in a Node.

Effects of Sharding:
------------------------
Sharding isolates the contention to specific entities. The entities decides the contention boundry.
In a Sharded system, Strong Consistency is achieved by: Isolating operations to a specific entity, which processes one message/request  at a time

Sharding , Consistency , Scalability :
---------------------------------------
1. Scalability is acheived by distributing shards across nodes.
2. Strong Consistency is acheived by isolating operation to specific entities.
3. Careful choice of shard keys is imp to maintain good scalability.
4. Sharding help reducing Contention. It does not eliminates it though.
5. Sharding is a CP solution so it sacrifies Availability.

Caching with Shards ;
----------------------------
Consistent image of data is cache and Db is essesntial.
So we update DB and write the same to Cache.
This makes we barely use DB for reading. We use that only for writitng.
Since most of the apps are read heavy,,this optimizes the app to greater extent.


Availaility and Scalability:
------------------------------
CRDT : Conflict Free Replicated DataTypes


CRDTs for Availabiity : At App level
----------------------------------------
CvRDT and CmRDT
CvRDT is widely supported.

CvRDT : Ensures High Availability and Eventual Consistency

With CRDTs write happns in all nodes or replicas. And eventually merged to the final State.

Merge operation must be Commutative , Associative , Idempotent(Duplicate processign should not impact our result).

You can create custom CRDTs as long as you can define a merge operation.


Effects of CRDTs;
=====================
Map , Set , Conts can be used for CRDT...Tombstone(marker to denote deleted data : This increases the CRDT size)

Dist Data is primarily an availability Solution which is Eventually Consistent.
Note : We mean Dist Data , not Dist System

Read/Write Consistency in Distributed Data allows you to: Tune your solution towards Consistency, but away from Availability.


Consistency or Availability:
------------------------------
CAP Theorem forces us to have either Consistency or Availability :
Choise must be made at Business Level not at technical level

Factor out with Doamin experts on : Unavailable vs Eventual Consistent.

Within the same application or system, we sometimes need consistency, and other times need availability, depending on the use case.




CQRS Pattern :
=======================================================
Make sure if that is necessary. It should not be an overkill for us.
CQRS makes system : highly Scalable and Resilient.

You should consider using CQRS/ES if: You need auditability , You need additional resilience or scalability.

State Based Persistence :
-----------------------------
Saving latest state in DB rather than the way how it reached that state.it means persisence of destination alone rather than the journey(intent)

Disadvantage:
-----------------
Only the latest state can be viewed.
The corrupted state cannot be fixed.


Event Sourcing:
-----------------
1. For new domain insights.
2. We can replay events from history.

When using State Based Persistence, in order to recover from errors in state, or evolve our domain we can: Persist an audit log alongside our state so we understand where the state came from. 

When an audit log is stored alongside the state, we must be wary of which of the following: The audit log and the state must be updated in a transactional way.

A snapshot should be created: When the time to restore from the log is becoming excessive.

If you need to evolve your data model, you should do so by: Creating a new version of the event

Command Sourcing : Persist the commands instead of events.
But commands must be idempotent.
We need to validate the commands before persisting them. 

the problem that we have encountered with these conflicting models is:
  A model that is optimized for writes (Commands) may not be optimized for reads (Queries).
  
  
CQRS : Command Query Responsiblity Seperation Model:
=====================================================
using the same DB model for read and write load is difficult sometimes.
The aggregates that we use for write load does not match the needs for read load. Would not be higly performant.

Most applications tend to have:  More reads than writes.

So idea is to write(commnds) in a write model9DB / Data store) and then create projections  as per the requirement in the read model(Query == In Denormalized form  : Think of Reservation on Custormer ID based.)
Each model can be independentaly optimized or scaled.


An ideal read model is often setup so the data can be read directly from a single table in the database, as is, with minimal additional processing/formatting.

Polyglot Persistence means: Using different databases within your system, depending on the use case

A key benefit to combining CQRS and Event Sourcing is:
 The presence of the event log allows all new projections to be retroactive.
 The read and write models are decoupled, allowing them to evolve independently.
 The read and write models can be optimized individually, depending on their needs.
 



SECURITY CHALLENGES UNIQUE TO THE CLOUD  : FROM IS APP DEV:
==================================================================
Nefarious users : lossing credentials in cloud
Insecure APi and interfaces
Data leakage via shared resources : SQL injection , Insufficient data access controls
Traffic Hijacking  : Gain access to data , prevent legitimate access to data. Man in the middle or man in the browser(cross site scripting)attack reads unencrypted data.
Malicious insider : Employees or Contractors or even the cloud vendor contractors : Prevent if by prevent access to data at rest , audit logs, auditing data access.
Privacy issue : Exposing PII data. using Canary record to identify ian attakcer has hijacked our system would help preventing this issue.


SSO in Cloud  :
====================
usually done via SAML and OpenID protocol
federated Identiy is a SSO implementaation to provision a single point of credential provider so that user does not have to login to each application every time.
Every app trusts the single Identity provider.
Proxy based and Portal based SSO .


Patterns for Scalable and resillient App : From GCP 
==========================================================
The flexibility to adjust the resources consumed by an app is a key business driver for moving to the cloud. 
Scalability: Adjusting capacity to meet demand
Tools  " Cloud Run  + GKE +Compute Engine with Auto scalers  +  Cloud Monitoring

Resilience: Designing to withstand failures
Deploying Apps across zones and regions



Drivers :
===========================================================
Business drivers:
-----------------------------------------------------------
make ur systems Uptine efective even under unbalanced load
Cost effective 
Increase availability
Time to market. be felxible


Development Drivers:
-----------------------------------------------------------
Minimize time spent investigating failures
Increase time spent on developing new features.
Minimize repetitive toil through automation.
Build apps using the latest industry patterns and practices.


Operations drivers:
-----------------------------------------------------------
Reduce the frequency of failures requiring human interventio
Increase the ability to automatically recover from failures.
Minimize the impact from the failure of any particular component.



Constraints:
=====================
Dependencies on hardware or software that is difficult to scale.
Lack of skills or experience in your development and operations teams.


Patterns and practices: https://cloud.google.com/solutions/scalable-and-resilient-apps
==========================================================================================
App lifecycle  : Infra design  + App Arch + Storage choices + deployment choices

Three themes are evident in the patterns::
Automation : Automating your infrastructure provisioning, testing, and app deployments increases consistency and speed, and minimizes human error.
             Create immutable infrastructure through automation to improve the consistency of your environments and increase the success of your deployments.
			 Infrastructure as code (IaC) is a technique that encourages you to treat your infrastructure provisioning and configuration in the same way you handle application code
			 IaC minimizes human error and improves the consistency and reproducibility of your apps and environments.
			 IaC increases the resilience of your apps.
			 Cloud Deployment Manager lets you automate the creation and management of Google Cloud resources with flexible templates.
			 Google Cloud also has built-in support for popular third-party IaC tools, including Terraform, Chef, and Puppet.
	Create immutable infrastructure : 		 leads to more predictable deployments and rollbacks. 
	                                         Immutable infrastructure mandates that resources never be modified after they're deployed. 
	                                         It also mitigates issues that are common in mutable infrastructures, like configuration drift and snowflake servers

			  
Loose coupling     : Treating your system as a collection of loosely coupled, independent components allows flexibility and resilience.



Data-driven design : Collecting metrics to understand the behavior of your app is critical.  
                     Decisions about when to scale your app, or whether a particular service is unhealthy, need to be based on data. 
					 Metrics and logs should be core features.


Design for high availability:
-------------------------------------
Physically distribute resources : Compute Engine managed service 
                                  GKE regional cluster
Favor managed services :  regional or Zonal availabuility or replicTION MODEL DEPENDS ON CERTAINI SERVICES.
Load-balance at each tier : For GKE and Compute engine we need to choose the load balancing . While for App Engine and Cloud Run LB is offered inherently.
                            GCP gives layer 4 and layer 7
Monitor your infrastructure and apps  : Use Cloud Monitoring for Inra Metrics gathering automatically. Many service does this automatically sending metrics data to CM.
                                        but CM does not ofer App level monitoring. We may use some third part tools for this.
    App monitoring	: How long it takes to execute a query. Use OpenCensus
    Service Monotoring : 	monitor the interactions between the different services and components in your apps.
	                        These metrics can help you diagnose problems like increased numbers of errors or latency between services.
					        Use Istio : for sending servie level metrics to CM.provides insights and operational control over your network of microservices.
                                        generates detailed telemetry for all service communications
    End-to-end monitoring :  also called black-box monitoring, it reveals availability as perceived by the user.
	
	
Balance cost and user experience : factors for Scaling Profile
-----------------------------------------------------------------
The cost-to-performance ratio might be different for a non-business-critical internal app where users are probably more tolerant of small delays. 
Hence, your scaling profile can be less aggressive. In this instance, keeping costs low might be of greater importance than optimizing the user experience.


Set baseline resources  :
--------------------------
extent of baseline resources is influenced by the type of app and traffic profile.


Configure autoscaling   :
--------------------------


Minimize startup time:
--------------------------
creating custom images and instance templates can increase your deployment speed,
it can also increase maintenance costs because the images might need to be updated more frequently. 


Containerize your app:
-----------------------------


Optimize your app for fast startup:
----------------------------------------


Favor modular architectures:
------------------------------------


Aim for statelessness:
----------------------------
A stateless app or service does not retain any local persistent data or state. 
A stateless model ensures that you can handle each request or interaction with the service independent of previous requests. 
This model facilitates scalability and recoverability, 
because it means that the service can grow, shrink, or be restarted without losing data that's required in order to handle any in-flight processes or requests.
Statelessness is especially important when you are using an autoscaler, because the instances, nodes, or pods hosting the service can be created and destroyed unexpectedly.

By ensuring clean separation of stateless and stateful services, you can ensure easy scalability for stateless services while adopting a more considered approach for stateful services.


Manage communication between services:
----------------------------------------------
Service mesh

Use appropriate database and storage technology:
------------------------------------------------------
NoSQL databases often choose availability over consistency.
Whether a NoSQL database is appropriate often comes down to the required degree of consistency. 
If your data model for a particular service does not require all the features of an RDBMS, and
can be designed to be eventually consistent, choosing a NoSQL database might offer increased availability and scalability.

Implement caching:
----------------------
Caching supports improved scalability by reducing reliance on disk-based storage. 
Because requests can be served from memory, request latencies to the storage layer are reduced, typically allowing your service to handle more requests.
In addition, caching can reduce the load on services that are downstream of your app, especially databases, allowing other components that interact with that downstream service to also scale more easily or at all.


Testing  :
-----------------
Resiliencey and load testing of your app.
Check the SRE book for all advice.

Peak-season production readiness.





SNOWFLAKE SERVERS:
===========================
This is more of the env inconsistency. This is addressed by IaC.
Where the operation configs are hosted in a file. Easy to read and MODIFY and track env changes in a single point.
both test and prod can be in sync.
We use tools like Puppet and Chef.
Any env with above anomilities is called as Snowflake Servers.







************************************************************************************************************************************************************************************************************************************
************************************************************************************************************************************************************************************************************************************
************************************************************************************************************************************************************************************************************************************
                                                                                     Google Cloud 
																			================================
																			
																			
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                                                    GENERAL NOTES
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
** We can manipulate the GCP res using gcloud SDK , API lib , Consle


** Cloud SDK must be installed in local machine to access a VM instance or any GCP service from local machine.
   Then run gcloud --commnd in the sdk console to manipulate the GCP resources from your local machine
   run glcoud init ==> to set up the project and do anything
   gcloud compute instance list
   gcloud compute ssh "instance_name"  ==> connects to the given VM instance using ssh putty
   
   
**  Always enable the API for any service which you need to access.


** gcloud compute scp --verbosity debug C:\Users\DELL\Desktop\Helson_21.txt instance-1:/home/helsoncloud
   gcloud compute scp --recurse instance-name:remote-dir local-dir
   To transfer files using SCP, you must have a firewall rule on the network that your instance uses that allows SSH connections on port 22
   
   
** We can package as Software in 3 ways :
Container Image
Boot Image
Startup Script

 












------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	
Check for Google Cloud Products:
----------------------------------
https://cloud.google.com/products

Phase 1: Experiment with a Microservices Deployement with the below products
------------------------------------------------------------------------------
Compute         : Compute Engine  + App Engine

Containers      : GKE  + GKE Monitoring

Databases       : BigTable + MemoryStore + Cloud SQL

API Management  : Apigee + Cloud Endpoints

Serverless      : Cloud Functions + Cloud Run

Networking      : Cloud Load Balancing + Cloud Monitoring + Cloud Logging


Security Challenges with Cloud  : IS APP Dev
=================================================
Characteristics of Cloud  :
-----------------------------
1. On demand and Self Service
2. Broad Network accesse
3. Resource pooling
4. Rapid Elasticity
5. measured Sercvice  : Does not have to be infinite in elastic. setting up threshold for service monitoring.


Cloud Service Model :
---------------------------
IaaS : 
=======
only hardware is provisioned and the network firewall things are taken care.
user has control over Apps and OS and Application Firewall and Runtime like Web or App server.


PaaS : Serverless
======================
Customer controls only the App and Secure Data Access Controls.
The OS and rest are taken care by the vendor.


SaaS:
===================
Highest Level of Cloud Service model.
Customer has just the control over the deployed application which can leverage the cloud vendor's tools and technologies.
Can do the config changes to the underlying application hosting environments.
More secured than the self managed one since we have dedicated team to do the security checks for the hosted software.


Deployement Model :
=======================
Private Cloud : Strong control over the env like static Ip assignment etc
Community Model : 
Public Cloud
Hybrid Cloud


Regions and zones:
=======================
A region is a specific geographical location where you can host your resources.
Regions have three or more zones.
For example, the us-west1 region denotes a region on the west coast of the United States that has three zones: us-west1-a, us-west1-b, and us-west1-c.
Regional and Zonal resources. ==> Reg res (static IP add,etc)can be used b any zones. But zonal res (zonl persistence disk)are restircted to specific zones.
Other resources, such as images, are global resources that can be used by any other resources across any location. 

Zone is connected to one or more clustres.

 Choosing a region and zone is important for several reasons:
 Handling failures :  In order to deploy fault-tolerant applications that have high availability, Google recommends deploying applications across multiple zones and multiple regions
 Decreased network latency : 
 
 India has only 3 zones under asia-south -region
 
 Quotas : Certain resources, such as static IPs, images, firewall rules, and VPC networks, have defined project-wide quota limits and per-region quota limits





 

Google Cloud products :
====================================================================================================================================================================================================================================================================================================================================================================================================================================================
                                                                                                        COMPUTE ENGINE  
====================================================================================================================================================================================================================================================================================================================================================================================================================================================

VIRTUAL MACHINE RUNNIG IN GOOGLE CLOUD DATACENTER

General purpose (E2, N1, N2, N2D) machines provide a good balance of price and performance
Compute optimized (C2) machines offer high-end vCPU performance for compute-intensive workloads
Memory optimized (M2) machines offer the highest memory and are great for in-memory databases
Accelerator optimized (A2) machines are based on the A100 GPU, for very demanding applications	

Choosing the right machine family and type:
-------------------------------------------------
General purpose - These machines balance price and performance and are suitable for most workloads including databases, development and testing environments, web applications, and mobile gaming.

Compute-optimized - These machines provide the highest performance per core on Compute Engine and are optimized for compute-intensive workloads, such as high performance computing (HPC), game servers, and latency-sensitive API serving.

Memory-optimized - These machines offer the highest memory configurations across our VM families with up to 12 TB for a single instance. They are well-suited for memory-intensive workloads such as large in-memory databases like SAP HANA and in-memory data analytics workloads.

Accelerator-optimized - These machines are based on the NVIDIA Ampere A100 Tensor Core GPU. With up to 16 GPUs in a single VM, these machines are suitable for demanding workloads like CUDA-enabled machine learning (ML) training and inference, and HPC. 


These are the avilable families  : Under each families we would many machine types.

We would have multiple VM instance running in one Host with Hypervisor. So for diff user each VM instance belongs to one user each. Just visualize it.



General purpose family :
---------------------------
E2 offers the lowest total cost of ownership.
Offer up to 32 vCPUs and 128GB of memory per node
E2 machine types also leverage dynamic resource management,
If you have workloads such as web serving, small-to-medium databases, and application development and testing environments 
that run well on N1 but don’t require large instance sizes, GPUs or local SSD, consider moving them to E2.
E2 has many variants like e2-standard-2 , 4,6,etc till 32 vCPu and 128GB
We also have micro edition like e2-micro , e2-small , e2-medium
These are a great fit for smaller workloads like micro-services or development environments that don’t require the full vCPU.


N2 introduced the 2nd Generation Intel Xeon Scalable Processors 
offer a greater than 20% price-performance improvement for many workloads and support up to 25% more memory per vCPU.
run at 2.8GHz base frequency, and 3.4GHz sustained all-core turbo, offer up to 80 vCPUs and 640GB of memory. 
This makes them a great fit for many general purpose workloads that can benefit from increased per core performance, 
Including web and application servers, enterprise applications, gaming servers, content and collaboration systems, and most databases.
Whether you are running a business critical database or an interactive web application, N2 VMs offer you the ability to get ~30% higher performance from your VMs,
N2 instances perform 2.82x faster than N1 instances on AI inference of a Wide & Deep model using Intel-optimized Tensorflow, 



N2D VMs are built on the latest 2nd Gen AMD EPYC (Rome) CPUs, and support the highest core count and memory of any general-purpose Compute Engine VM
same features as N2 VMs including local SSD, custom machine types, and transparent maintenance through live migration.
N2D VMs provide performance improvements for data management workloads that leverage AMD’s HIGHER MEMORY BANDWIDTH and higher per-system throughput 
with up to 224 vCPUs, making them the largest general purpose VM on Google Compute Engine.
N2D machine types are suitable for web applications, databases, workloads, and video streaming.
USEFUL FOR STREAMING , making them a great fit for memory bandwidth-hungry applications.



N1s are first-generation general purpose VMs and offer up to 96 vCPUs and 624GB of memory 
For GPU workloads, N1 supports a variety of NVIDIA GPUs



Second generation is highly recommneded.

For flexibility, general purpose machines come as predefined (with a preset number of vCPUs and memory), or can be configured as custom machine types
Custom machine types allow you to independently configure CPU and memory to find the right balance for your application,




Compute-optimized (C2) family:
----------------------------------
Compute-optimized machines focus on the highest performance per core and the most consistent performance to support real-time applications performance needs. 
offering up to 3.8 GHz sustained all-core turbo, these VMs are optimized for compute-intensive workloads such as HPC, gaming (AAA game servers), and high-performance web serving.
produce a greater than 40% performance improvement compared to the previous generation N1
offer higher performance per thread and isolation for latency-sensitive workloads
Compute-optimized VMs come in different shapes ranging from 4 to 60 vCPUs, and offer up to 240 GB of memory.
You can choose to attach up to 3TB of local storage to these VMs for applications that require higher storage performance. 
Whether you are optimizing for the number of queries per second or the throughput of your map routing algorithms
 
 
 
 
Memory-optimized (M1, M2) family :
--------------------------------------
Memory-optimized machine types offer the highest memory in our VM family
from 1TB to 12TBs of memory, and offer up to 416 vCPUs,
these VMs offer the most compute and memory resources of any Compute Engine VM offering.
They are well suited for large in-memory databases such as SAP HANA, as well as in-memory data analytics workloads
M1 VMs offer up to 4TB of memory, while M2 VMs support up to 12TB of memory.
making them a great choice for workloads that utilize higher memory configurations with low compute resources requirements.




Accelerator-optimized (A2) family:
-----------------------------------------
designed to meet today’s most demanding applications such as machine learning and HPC. 
A2 VMs were the first NVIDIA Ampere A100 Tensor Core GPU-based offering on a public cloud.
Each A100 GPU offers up to 20x the compute performance compared to the previous generation GPU and comes with 40GB of high-performance HBM2 GPU memory.



Instance Template  :
--------------------------
Use thatto quickly create VM instance with predefined config also for MIG (Managed Instance Groups)




Performance-driven dynamic resource management in E2 VMs :
-------------------------------------------------------------
In particular, the consistent performance delivered by E2 VMs is enabled by:

An evolution toward large, efficient physical servers
Intelligent VM placement
Performance-aware live migration
A new hypervisor CPU scheduler

Together we call these technologies dynamic resource management

In dynamic res mangmt : The Virtual CPU  (vCPU) is mapped to physical CPU.
So if the vCPU need is not there then the phy.CPu would be assigned to otjer vCPU
So that we can effectively use the physcial resources dynamically and create more VM instance on lesser hardwar. There are alredy used for most of th goole services.
But phy.res allocation would take time at some instance for the phy.res to get free. This is wait time.
The wait time is mitigated as follows.

An evolution toward large, efficient physical servers : live migration to new hardware
Intelligent VM placement : finding the best location to add VM
Performance-aware live migration : we can use live migration to transparently shift E2 load to other hosts in the data center.
A new hypervisor CPU scheduler : The new scheduler provides sub-microsecond average wake-up latencies and extremely fast context switching.



Instance groups:
--------------------------------

Let you operate apps on multiple IDENTICAL VMs. Vms with same configs.

Managed Instance Groups : MIG services, including: autoscaling, autohealing, regional (multiple zone) deployment, and automatic updating.
Unmanaged instance groups let you load balance across a fleet of VMs that you manage yourself.

Managed instance groups (MIGs) are suitable for scenarios like these:
Stateless serving workloads, such as a website frontend
Stateless batch, high-performance, or high-throughput compute workloads, such as image processing from a queue
Stateful applications, such as databases, legacy applications, and long-running batch computations with checkpointing

MIG can be Regional or Zonal  : Regional has high vailability , Application-based autohealing,  Regional (multiple zone) coverage, Load balancing.
								Scalability,  Automated updates of your software.
								Support for stateful workloads like  such as databases, DNS servers, legacy monolith applications, or long-running batch computations with checkpointing.
								Stateful MIGs preserve each instance's unique state (instance name, attached persistent disks, and metadata) on machine restart, recreation, auto-healing, or update.
								
MIG can be created using Instance Templates.

Auto healing based on application state by doing health check. Noit just relying on running or failed state of VM instance.
health checking signal that detects application-specific issues such as freezing, crashing, or overloading.
If a health check determines that an application has failed on a VM, the group automatically recreates that VM instance.
Health checking for autohealing causes MIGs to proactively replace failing instances, so this health check should be more conservative than a load balancing health check.
If Auto Healing is not selected, Vms are recreated only when the Vm is stopped.

A zonal MIG, which deploys instances to a single zone.
A regional MIG, which deploys instances to multiple zones across the same region. 
regional MIGs offer more capacity, with a maximum of 2,000 instances per regional group.
use a regional group to protect against a zonal failure


***Autoscaling policies include scaling based on CPU utilization, load balancing capacity, Cloud Monitoring metrics, or, for zonal MIGs, by using a queue-based workload like Pub/Sub.
Auto scaling is based on Auto Scale policies : CPU utilization is the most used one. 


# of Vms =  Sum of Actual CPU load
           ------------------------
		      Target CPU load  
(in percentage like 60% : So if the CPu utlization goes beyond 60% then respective number of VM instance is launched to meet target load VM needs)
** So lesser the targte load, then more VMs are launched.



** Auto Updater is used for rolling out app change of versions without affecting the user faing systemn down time Useful for rolling updates and canary testing.
1. Decide how many machine you want the updates to be applied.
2. How many instance can be down while updating.
3. Action  : restart or recreate : recreate a new Vm instance from the template or, restart the same instance with the updated template
4. Update rollout Mode : Proactive  (Does the default rollout) . 
          Oppurtunistic : creates a new instance with the updated version only when the system scales up. Else only the old version is served
		  The old Vm instances are deleted when we need to scale down.
5. Vm instance can be ReCreated or ReStarted.
		  
		  
		  
Diff between MAX_UNAVAILABLE & MAX_SURGE
----------------------------------------------


		  
Rolling Update  : 
----------------------
Control how many VM updated at a time.
Mode  : ProActive.
Update only certain number of Vm at a time.
Do the health check.
Dont route the traffic to the updated instance untill they ensure running fine.

Canary Update  :
----------------------
Useful for A/B testing.
New template 30% 
Old template 70%


The ratio is fixed and maintained. The ratio is maintained even when the Vm instances scales.
So do run the new template in production and collect statistics and apply the same for rest iof the Vm when the prod results are ok.


Faster rollout without reduction in services:
Go for this approach when you dont wnat any of the runnnig instances to be unavailbale due to update.
New template to 100%
Max unavilable Vm  : 0 : So no existing Vms are updated. Instead new instances are created with the new template.
Max surge to 100%.
Create 100% extra Vm ins the new template .
Update all instances at a time. procative mode of deployment.
Remove the old Vm only when the new VM pass the health check.


How updater works with Autohealing
---------------------------------------
Say like Max unavailable is 2
tghen only 2 instances are expected to unavailable while update.
lets say one of the running Vm with the old template goes down after new temp is applied to 2 existing instaces.
So for the second rollout batch , including one of the old Vm whihc is down, then updater simply apply the new changes only to one VM, to maintain the max unavailable count whihc is 2 in this case.
If 2 Vms are auto healed no new updates are applied. Updater waits for the autohealing of theose 2 Vms to be completed.



How Updater and Autoscaling works together:
------------------------------------------------
The auo scler comes into picture when the load increases with the existing VM.
So the auoscler adds new Vm to meet the current load demand.
However when we do some update to the existing 2 VMs, the load on the other eixtsing VMs increases. 
but the auot scaler si aware that it is due to the update rllout that the load is spiking up, so it would wait for thw rollout to be completed and would not add any new instances until the rollout completes.
Sp Updater and AutoScaler wokr s well together in this case.



Updater evenly rollsout new version across the zones.Also it removes them evenly.
---------------------------------------------------------------------------------------


Updater and Load balancer:
-----------------------------
Worried about interuppting live user session?
No new tarrfic are arouted to the existing Vm which is to be updated.
The LB waits for live session to be complted and then wiats until the connection drain timeout (configured in the LB)
Then it recerates a new VM instance and send the traffic to that updated instance.


AutoScaling based on the task Queue:
--------------------------------------------
For batch we have producers and workers(like spring worker or processors)
Each worker runs in a VM instances.
the number of worker is decided dynamically by the Auo Scaler based on the number of items or task to be processed
The Auto scaler shuts down all running Vm instances when no more task is pending to be processsed.


		  

Container Optimized OS is pre-installed with Docker. So you can menton the image name in the instance template so each instance in the MIG would be started with the container running.
                 			
							
							
Unmanaged instance groups :
--------------------------------
Unmanaged instance groups can contain heterogeneous instances that you can arbitrarily add and remove from the group		
Unmanaged instance groups do not offer autoscaling, autohealing, rolling update support, multi-zone support, or the use of instance templates and are not a good fit for deploying highly available and scalable workloads.
								
 Use unmanaged instance groups if you need to apply load balancing to groups of heterogeneous instances, or if you need to manage the instances yourself.



====================================================================================================================================================================================================================================================================================================================================================================================================================================================
                                                                                                        CLOUD RUN  
====================================================================================================================================================================================================================================================================================================================================================================================================================================================
Cloud Run is the serverless compute service in GCP.
It is primarily used to run cloud native Containarized apps.
So any language code can be used to run those containers in Cloud Runtime.
Cloud Code is an IDE plugin used to test locally and deploy remote of a containeraized app in cloud run. It creates a loal env with Scaffold and miniKube to test the containarized app locally.
It uses either Dockerfile or BuildPacks (used to create a container out of an app without DockerFile) to create a Container .


Cloud Run is available in two flavours : Fully Managed Cloud Run and Cloud Run on Anthos(hybrid cloud service)
It simply Auto Scales.
Pay per use.
it providea a free https endpoint. Also we can attach our service to our own custom domain.

Cloud Run has a limit of (1 hr ) in request processing limit. Beyond that U would get timeExceed exception.







====================================================================================================================================================================================================================================================================================================================================================================================================================================================
                                                                                                        GKE
====================================================================================================================================================================================================================================================================================================================================================================================================================================================
Open items:
-----------------
How k8s behave in multitennant env. Do we have to create new cluster for each app team? 
What is the use of namespace.
Do we need new deployments for new apps.Can we add a new item to the existing deplyment.
Types of deployment strategies.
What is OCI (Open Container Interface). Your container in k8s must be compatible to this interface.




We have different release channels to manage and upgrade the Kubernetes cluster namely : Rapid  --> Regular --> Stable


Connect to GKE cluster  :
---------------------------
gcloud container clusters get-credentials helson-kube-cluster1 --zone us-central1-c --project applied-pursuit-282708


Create Deployment  :
----------------------
kubectl create deployment kube-deploy1 --image=helsonkumar/spring:spring_test


Expose the deployment via a port :
------------------------------------
kubectl expose deployment kube-deploy1 --port=8080 --type=LoadBalancer

kubectl get pods -o wide

kubectl get replicaset

Scale the pods :
--------------------------------------
kubectl scale deployment deply_name --replicas=3


Delete a pod :
------------------
kubectl delete pod pod-name


Set a diff image from V1 to V2 in the ruuning replica set :
-------------------------------------------------------------
kubectl set image deployment deplymnt_name container_name=docker_acc/repo_name:container_tag

k8s would create a new replicaset for the newly set image.
If it is valid image then the new rs would be effective and the old rs would be shutdown .
If it is invalid then the old rs would still remain , but the new rs would be of zero running instances.

Deloyment ==> on top we have RS ==> on top we have Pods

A service is created when we expose our deployment.
This holds a static IP to the external world . Wheras the pods get a diff Ip each.


kubectl get services

kubectl get componentstatuses


Rollouts :
=================
kubectl rollout history deployment dep-name

kubectl rollout undo deployment deply-name --to-revision=revision-number
(This would restore the app version to the respective revisions)

kubectl get deployment hello-world-1 -o yaml


Describe  :
=================
kubectl describe pod_name
would give u the name of the node in whihc pods runs, its container ,etc



Multi Container Pods:
=========================
We use PODs instead of Containers in K8s bcz PODs is a wrapper and does some extra work like Liveness-checking probe and decide on Restart Policy.

Running multiple container within a POD.
In pre-container world this is equivalent to multiple process runnning in same server.
These container are tightly coupled.
They share the same IP and POTRT namspace.

Then why cannot we bundle all process into one Container.?????
"One Process Per Container" is the core principle.

Else it would make it harder for troubleshooting . Logs from all process would be intermixed.
So it would help in reutlizing the granualr containier by ther process. So always make it decoupled.


Use Cases for Multi-Container Pods :
========================================
The primary purpose of a multi-container Pod is to support co-located, co-managed helper processes for a primary application
Sidecar containers
Proxies, bridges, and adapters

While you can host a multi-tier application (such as WordPress) in a single Pod, the recommended way is to use separate Pods for each tier
for the simple reason that you can scale tiers up independently and distribute them across cluster nodes.

Communication between containers in a Pod:
Shared volumes in a Kubernetes Pod . This is Kubernetes volume.
They last until the POD stays alive.


Inter Process Communication : Can be done via POSIX Shared memory stuffs.
like Common queue for writer and reader container.

But cons is : The container may start in unordered fashion

























