Building Scalable Systems:
=========================================================
All systems are eventually consistent.

Performance has limit
Scalability has no limits.

Eventul consistency : If no updates then all part of the system would be eventually consistent.
Types: Consual Consistent , Strong Consistent , Sequential Consistent

All systems are eventual consistent in natre . Think of GitHub code locks.

Using locks would help us achieve Strong Consistency. Think of Single Db accessed by amny parts of the system.
But lock adds more overhead like contention.
Strogn consistency is all about diff parts of the system responding with the same answer if ased at any time.
All parts must agree upon the state and respond with answer like group of Jury.
Strong consistency mean we dont wnat to have STALE DATA.

Contention :  Diff parts of system contending for a shared resource like DB record,this would lead toa state where more the load is , higher the time taken to release the resource.


Consistency and Availability cannot co-exist.

Law of Scalabilty:
=====================
Amdahl's Law : Contention Limits parallelization. Depicts the relation between Concurrenty and Throughput of the system.

Defines the limits of parallelization.
Piece of code we can do nothing about is the one whihc holds contention and which cannot be parallelized.


Coherency Delay :
======================
Think of meeting scheduling scenarios.

**** Concurrency Vs Throughput ****

Gunther's Law of Coherency Delay :
-----------------------------------
Accounts for Contention + Coherency Delay.

State of Coherence : Can be achieved by having members of the system agreeing upon the state change of each other. 
                     But this would result in a delay.This is called Coherency delay.
					 
Gunther's Law states that increasing concurrency can result in negative result due to Coherency Delay + Contention.

Linear Scalability is almost not achievable : Considering the communication and Contention and Coherence verhead between the parts of the system.

Stateless : Cannot be achived. Sharing state between parts of system losses the ability of Linear Scalability.


How Reactive System / Microservice enhances Scalability :
===========================================================
Avoids Contention by :
    Isolating Locks,Elimates Txn , Avoids Blocking operation.
	
Avoids Coherency-Delay by :
    Embracing eventual consistency 
	Building Autonomy ==> Systems which do not have to interact
	
This simply reduces them and not avoids them.


CAP Theorem:
===============================================================
Consistency Availability Partition Tolerance

CA side is not a valid in kost cases....either CP or AP is possible..

Partition tolerance : When part of the system goes  down and unable to commnicate with each other the system still remains respnsive.
Partition does not mean the data partition. It means the node or system unvailability scenario.

Dealing with Partition : Making the system to remain responsive even under the case of drop of n/w comm
============================================================================================================
1. CP : Dropping the system which has gone down. So we write to one node and have that for read as well. Here the consistency is achived but if that single node goes down then we loss availability.
2. AP : Start writing on both the nodes. Here availability is achived , but consistency of data is achived since the data on both nodes would remain inconsistent.We will have to merge data after partition is resolved.

In reality, most systems balance the Consistency & Availability concerns usually favoring one side or the
other.

By doing all reads and writes through a single master node, we can guarantee: Consistency
When we failover to a replica, we are sacrificing: Consistency

By allowing reads and writes to go to any single database node, this system is favouring: Availability
Think of writig to all replicas before txn completes.... : Here we compromise availability...


Contention Vs Scalability:
============================
Consistency creates contention.
Contention will result in demnishing result in Scalability

Isolating  Contention :
============================
In Reactive Apps Shardign at APP level ;; Not the Db level
Record level locking instead of table level locking.

To provide scalability, while maintaining Consistency we look to: Isolate resource of contention

Sharding for Consistency : Provides Strong consistency
----------------------------------------------------------
Nodes has many Shards  ==> A Shard consist of many Entities.


A shard cannot exist in more than one node.
An entity cannot locate in more than one shard.

Query for an entitiy is decided by entity ID.
We use HashCode algo on entity ID to decide the shard to which that entity belongs.
Choosing correct enity ID is very important.

Sharding at the application level provides which of the following benefits:
----------------------------------------------------------------------------
Here even if the Db does not supports sharding we can still do sharding at app level.
Reduced comm between App and DB.
Strong Consistency
Improved Scalability.

We can describe Sharding by saying that: Sharding partitions(Nodes) entities or actors in the domain according to their unique Id. correct

Sharding Co-ordinator will ensure proper routing of the Entity request.

Thumb Rule is to get 10 Shards in a Node.

Effects of Sharding:
------------------------
Sharding isolates the contention to specific entities. The entities decides the contention boundry.
In a Sharded system, Strong Consistency is achieved by: Isolating operations to a specific entity, which processes one message/request  at a time

Sharding , Consistency , Scalability :
---------------------------------------
1. Scalability is acheived by distributing shards across nodes.
2. Strong Consistency is acheived by isolating operation to specific entities.
3. Careful choice of shard keys is imp to maintain good scalability.
4. Sharding help reducing Contention. It does not eliminates it though.
5. Sharding is a CP solution so it sacrifies Availability.

Caching with Shards ;
----------------------------
Consistent image of data is cache and Db is essesntial.
So we update DB and write the same to Cache.
This makes we barely use DB for reading. We use that only for writitng.
Since most of the apps are read heavy,,this optimizes the app to greater extent.


Availaility and Scalability:
------------------------------
CRDT : Conflict Free Replicated DataTypes


CRDTs for Availabiity : At App level
----------------------------------------
CvRDT and CmRDT
CvRDT is widely supported.

CvRDT : Ensures High Availability and Eventual Consistency

With CRDTs write happns in all nodes or replicas. And eventually merged to the final State.

Merge operation must be Commutative , Associative , Idempotent(Duplicate processign should not impact our result).

You can create custom CRDTs as long as you can define a merge operation.


Effects of CRDTs;
=====================
Map , Set , Conts can be used for CRDT...Tombstone(marker to denote deleted data : This increases the CRDT size)

Dist Data is primarily an availability Solution which is Eventually Consistent.
Note : We mean Dist Data , not Dist System

Read/Write Consistency in Distributed Data allows you to: Tune your solution towards Consistency, but away from Availability.


Consistency or Availability:
------------------------------
CAP Theorem forces us to have either Consistency or Availability :
Choise must be made at Business Level not at technical level

Factor out with Doamin experts on : Unavailable vs Eventual Consistent.

Within the same application or system, we sometimes need consistency, and other times need availability, depending on the use case.




CQRS Pattern :
=======================================================
Make sure if that is necessary. It should not be an overkill for us.
CQRS makes system : highly Scalable and Resilient.

You should consider using CQRS/ES if: You need auditability , You need additional resilience or scalability.

State Based Persistence :
-----------------------------
Saving latest state in DB rather than the way how it reached that state.it means persisence of destination alone rather than the journey(intent)

Disadvantage:
-----------------
Only the latest state can be viewed.
The corrupted state cannot be fixed.


Event Sourcing:
-----------------
1. For new domain insights.
2. We can replay events from history.

When using State Based Persistence, in order to recover from errors in state, or evolve our domain we can: Persist an audit log alongside our state so we understand where the state came from. 

When an audit log is stored alongside the state, we must be wary of which of the following: The audit log and the state must be updated in a transactional way.

A snapshot should be created: When the time to restore from the log is becoming excessive.

If you need to evolve your data model, you should do so by: Creating a new version of the event

Command Sourcing : Persist the commands instead of events.
But commands must be idempotent.
We need to validate the commands before persisting them. 

the problem that we have encountered with these conflicting models is:
  A model that is optimized for writes (Commands) may not be optimized for reads (Queries).
  
  
CQRS : Command Query Responsiblity Seperation Model:
=====================================================
using the same DB model for read and write load is difficult sometimes.
The aggregates that we use for write load does not match the needs for read load. Would not be higly performant.

Most applications tend to have:  More reads than writes.

So idea is to write(commnds) in a write model9DB / Data store) and then create projections  as per the requirement in the read model(Query == In Denormalized form  : Think of Reservation on Custormer ID based.)
Each model can be independentaly optimized or scaled.


An ideal read model is often setup so the data can be read directly from a single table in the database, as is, with minimal additional processing/formatting.

Polyglot Persistence means: Using different databases within your system, depending on the use case

A key benefit to combining CQRS and Event Sourcing is:
 The presence of the event log allows all new projections to be retroactive.
 The read and write models are decoupled, allowing them to evolve independently.
 The read and write models can be optimized individually, depending on their needs.
 



SECURITY CHALLENGES UNIQUE TO THE CLOUD  : FROM IS APP DEV:
==================================================================
Nefarious users : lossing credentials in cloud
Insecure APi and interfaces
Data leakage via shared resources : SQL injection , Insufficient data access controls
Traffic Hijacking  : Gain access to data , prevent legitimate access to data. Man in the middle or man in the browser(cross site scripting)attack reads unencrypted data.
Malicious insider : Employees or Contractors or even the cloud vendor contractors : Prevent if by prevent access to data at rest , audit logs, auditing data access.
Privacy issu : Exposing PII data. using Canary record to identify ian attakcer has hijacked our system would help preventing this issue.


SSO in Cloud  :
====================
usually done via SAML and OpenID protocol
federated Identiy is a SSO implementaation to provision a single point of credential provider so that user does not have to login to each application every time.
Every app trusts the single Identity provider.
Proxy based and Portal based SSO .










************************************************************************************************************************************************************************************************************************************
************************************************************************************************************************************************************************************************************************************
************************************************************************************************************************************************************************************************************************************
                                                                                     Google Cloud 
																			================================
																			
																			
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                                                    GENERAL NOTES
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
** We can manipulate tge GCP res using gcloud SDK , API lib , Consle


** Cloud SDK must be installed in local machine to access a VM instance or any GCP service from local machine.
   Then run gcloud --commnd in the sdk console to manipulate the GCP resources from your local machine
   run glcoud init ==> to set up the project and do anything
   gcloud compute instance list
   gcloud compute ssh "instance_name"  ==> connects to the given VM instance using ssh putty
   
   
**  Always enable the API for any service which you need to access.


** gcloud compute scp --verbosity debug C:\Users\DELL\Desktop\Helson_21.txt instance-1:/home/helsoncloud
   gcloud compute scp --recurse instance-name:remote-dir local-dir
   To transfer files using SCP, you must have a firewall rule on the network that your instance uses that allows SSH connections on port 22
   
   
** We can package as Software in 3 ways :
Container Image
Boot Image
Startup Script

 












------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	
Check for Google Cloud Products:
----------------------------------
https://cloud.google.com/products

Phase 1: Experiment with a Microservices Deployement with the below products
------------------------------------------------------------------------------
Compute         : Compute Engine  + App Engine

Containers      : GKE  + GKE Monitoring

Databases       : BigTable + MemoryStore + Cloud SQL

API Management  : Apigee + Cloud Endpoints

Serverless      : Cloud Functions + Cloud Run

Networking      : Cloud Load Balancing + Cloud Monitoring + Cloud Logging


Security Challenges with Cloud  : IS APP Dev
=================================================
Characteristics of Cloud  :
-----------------------------
1. On demand and Self Service
2. Broad Network accesse
3. Resource pooling
4. Rapid Elasticity
5. measured Sercvice  : Does not have to be infinite in elastic. setting up threshold for service monitoring.


Cloud Service Model :
---------------------------
IaaS : 
=======
only hardware is provisioned and the network firewall things are taken care.
user has control over Apps and OS and Application Firewall and Runtime like Web or App server.


PaaS : Serverless
======================
Customer controls only the App and Secure Data Access Controls.
The OS and rest are taken care by the vendor.


SaaS:
===================
Highest Level of Cloud Service model.
Customer has just the control over the deployed application which can leverage the cloud vendor's tools and technologies.
Can do the config changes to the underlying application hosting environments.
More secured than the self managed one since we have dedicated team to do the security checks for the hosted software.


Deployement Model :
=======================
Private Cloud : Strong control over the env like static Ip assignment etc
Community Model : 
Public Cloud
Hybrid Cloud


Regions and zones:
=======================
A region is a specific geographical location where you can host your resources.
Regions have three or more zones.
For example, the us-west1 region denotes a region on the west coast of the United States that has three zones: us-west1-a, us-west1-b, and us-west1-c.
Regional and Zonal resources. ==> Reg res (static IP add,etc)can be used b any zones. But zonal res (zonl persistence disk)are restircted to specific zones.
Other resources, such as images, are global resources that can be used by any other resources across any location. 

Zone is connected to one or more clustres.

 Choosing a region and zone is important for several reasons:
 Handling failures :  In order to deploy fault-tolerant applications that have high availability, Google recommends deploying applications across multiple zones and multiple regions
 Decreased network latency : 
 
 India has only 3 zones under asia-south -region
 
 Quotas : Certain resources, such as static IPs, images, firewall rules, and VPC networks, have defined project-wide quota limits and per-region quota limits





 

Google Cloud products :
====================================================================================================================================================================================================================================================================================================================================================================================================================================================
                                                                                                        COMPUTE ENGINE  
====================================================================================================================================================================================================================================================================================================================================================================================================================================================

VIRTUAL MACHINE RUNNIG IN GOOGLE CLOUD DATACENTER

General purpose (E2, N1, N2, N2D) machines provide a good balance of price and performance
Compute optimized (C2) machines offer high-end vCPU performance for compute-intensive workloads
Memory optimized (M2) machines offer the highest memory and are great for in-memory databases
Accelerator optimized (A2) machines are based on the A100 GPU, for very demanding applications	

Choosing the right machine family and type:
-------------------------------------------------
General purpose - These machines balance price and performance and are suitable for most workloads including databases, development and testing environments, web applications, and mobile gaming.

Compute-optimized - These machines provide the highest performance per core on Compute Engine and are optimized for compute-intensive workloads, such as high performance computing (HPC), game servers, and latency-sensitive API serving.

Memory-optimized - These machines offer the highest memory configurations across our VM families with up to 12 TB for a single instance. They are well-suited for memory-intensive workloads such as large in-memory databases like SAP HANA and in-memory data analytics workloads.

Accelerator-optimized - These machines are based on the NVIDIA Ampere A100 Tensor Core GPU. With up to 16 GPUs in a single VM, these machines are suitable for demanding workloads like CUDA-enabled machine learning (ML) training and inference, and HPC. 


These are the avilable families  : Under each families we would many machine types.

We would have multiple VM instance running in one Host with Hypervisor. So for diff user each VM instance belongs to one user each. Just visualize it.



General purpose family :
---------------------------
E2 offers the lowest total cost of ownership.
Offer up to 32 vCPUs and 128GB of memory per node
E2 machine types also leverage dynamic resource management,
If you have workloads such as web serving, small-to-medium databases, and application development and testing environments 
that run well on N1 but don’t require large instance sizes, GPUs or local SSD, consider moving them to E2.
E2 has many variants like e2-standard-2 , 4,6,etc till 32 vCPu and 128GB
We also have micro edition like e2-micro , e2-small , e2-medium
These are a great fit for smaller workloads like micro-services or development environments that don’t require the full vCPU.


N2 introduced the 2nd Generation Intel Xeon Scalable Processors 
offer a greater than 20% price-performance improvement for many workloads and support up to 25% more memory per vCPU.
run at 2.8GHz base frequency, and 3.4GHz sustained all-core turbo, offer up to 80 vCPUs and 640GB of memory. 
This makes them a great fit for many general purpose workloads that can benefit from increased per core performance, 
Including web and application servers, enterprise applications, gaming servers, content and collaboration systems, and most databases.
Whether you are running a business critical database or an interactive web application, N2 VMs offer you the ability to get ~30% higher performance from your VMs,
N2 instances perform 2.82x faster than N1 instances on AI inference of a Wide & Deep model using Intel-optimized Tensorflow, 



N2D VMs are built on the latest 2nd Gen AMD EPYC (Rome) CPUs, and support the highest core count and memory of any general-purpose Compute Engine VM
same features as N2 VMs including local SSD, custom machine types, and transparent maintenance through live migration.
N2D VMs provide performance improvements for data management workloads that leverage AMD’s HIGHER MEMORY BANDWIDTH and higher per-system throughput 
with up to 224 vCPUs, making them the largest general purpose VM on Google Compute Engine.
N2D machine types are suitable for web applications, databases, workloads, and video streaming.
USEFUL FOR STREAMING , making them a great fit for memory bandwidth-hungry applications.



N1s are first-generation general purpose VMs and offer up to 96 vCPUs and 624GB of memory 
For GPU workloads, N1 supports a variety of NVIDIA GPUs



Second generation is highly recommneded.

For flexibility, general purpose machines come as predefined (with a preset number of vCPUs and memory), or can be configured as custom machine types
Custom machine types allow you to independently configure CPU and memory to find the right balance for your application,




Compute-optimized (C2) family:
----------------------------------
Compute-optimized machines focus on the highest performance per core and the most consistent performance to support real-time applications performance needs. 
offering up to 3.8 GHz sustained all-core turbo, these VMs are optimized for compute-intensive workloads such as HPC, gaming (AAA game servers), and high-performance web serving.
produce a greater than 40% performance improvement compared to the previous generation N1
offer higher performance per thread and isolation for latency-sensitive workloads
Compute-optimized VMs come in different shapes ranging from 4 to 60 vCPUs, and offer up to 240 GB of memory.
You can choose to attach up to 3TB of local storage to these VMs for applications that require higher storage performance. 
Whether you are optimizing for the number of queries per second or the throughput of your map routing algorithms
 
 
 
 
Memory-optimized (M1, M2) family :
--------------------------------------
Memory-optimized machine types offer the highest memory in our VM family
from 1TB to 12TBs of memory, and offer up to 416 vCPUs,
these VMs offer the most compute and memory resources of any Compute Engine VM offering.
They are well suited for large in-memory databases such as SAP HANA, as well as in-memory data analytics workloads
M1 VMs offer up to 4TB of memory, while M2 VMs support up to 12TB of memory.
making them a great choice for workloads that utilize higher memory configurations with low compute resources requirements.




Accelerator-optimized (A2) family:
-----------------------------------------
designed to meet today’s most demanding applications such as machine learning and HPC. 
A2 VMs were the first NVIDIA Ampere A100 Tensor Core GPU-based offering on a public cloud.
Each A100 GPU offers up to 20x the compute performance compared to the previous generation GPU and comes with 40GB of high-performance HBM2 GPU memory.



Instance Template  :
--------------------------
Use thatto quickly create VM instance with predefined config also for MIG (Managed Instance Groups)




Performance-driven dynamic resource management in E2 VMs :
-------------------------------------------------------------
In particular, the consistent performance delivered by E2 VMs is enabled by:

An evolution toward large, efficient physical servers
Intelligent VM placement
Performance-aware live migration
A new hypervisor CPU scheduler

Together we call these technologies dynamic resource management

In dynamic res mangmt : The Virtual CPU  (vCPU) is mapped to physical CPU.
So if the vCPU need is not there then the phy.CPu would be assigned to otjer vCPU
So that we can effectively use the physcial resources dynamically and create more VM instance on lesser hardwar. There are alredy used for most of th goole services.
But phy.res allocation would take time at some instance for the phy.res to get free. This is wait time.
The wait time is mitigated as follows.

An evolution toward large, efficient physical servers : live migration to new hardware
Intelligent VM placement : finding the best location to add VM
Performance-aware live migration : we can use live migration to transparently shift E2 load to other hosts in the data center.
A new hypervisor CPU scheduler : The new scheduler provides sub-microsecond average wake-up latencies and extremely fast context switching.



Instance groups:
--------------------------------

Let you operate apps on multiple IDENTICAL VMs. Vms with same configs.

Managed Instance Groups : MIG services, including: autoscaling, autohealing, regional (multiple zone) deployment, and automatic updating.
Unmanaged instance groups let you load balance across a fleet of VMs that you manage yourself.

Managed instance groups (MIGs) are suitable for scenarios like these:
Stateless serving workloads, such as a website frontend
Stateless batch, high-performance, or high-throughput compute workloads, such as image processing from a queue
Stateful applications, such as databases, legacy applications, and long-running batch computations with checkpointing

MIG can be Regional or Zonal  : Regional has high vailability , Application-based autohealing,  Regional (multiple zone) coverage, Load balancing.
								Scalability,  Automated updates of your software.
								Support for stateful workloads like  such as databases, DNS servers, legacy monolith applications, or long-running batch computations with checkpointing.
								Stateful MIGs preserve each instance's unique state (instance name, attached persistent disks, and metadata) on machine restart, recreation, auto-healing, or update.
								
MIG can be created using Instance Templates.

Auto healing based on application state by doing health check. Noit just relying on running or failed state of VM instance.
health checking signal that detects application-specific issues such as freezing, crashing, or overloading.
If a health check determines that an application has failed on a VM, the group automatically recreates that VM instance.
Health checking for autohealing causes MIGs to proactively replace failing instances, so this health check should be more conservative than a load balancing health check.
If Auto Healing is not selected, Vms are recreated only when the Vm is stopped.

A zonal MIG, which deploys instances to a single zone.
A regional MIG, which deploys instances to multiple zones across the same region. 
regional MIGs offer more capacity, with a maximum of 2,000 instances per regional group.
use a regional group to protect against a zonal failure


***Autoscaling policies include scaling based on CPU utilization, load balancing capacity, Cloud Monitoring metrics, or, for zonal MIGs, by using a queue-based workload like Pub/Sub.
Auto scaling is based on Auto Scale policies : CPU utilization is the most used one. 


# of Vms =  Sum of Actual CPU load
           ------------------------
		      Target CPU load  
(in percentage like 60% : So if the CPu utlization goes beyond 60% then respective number of VM instance is launched to meet target load VM needs)
** So lesser the targte load, then more VMs are launched.



** Auto Updater is used for rolling out app change of versions without affecting the user faing systemn down time Useful for rolling updates and canary testing.
1. Decide how many machine you want the updates to be applied.
2. How many instance can be down while updating.
3. Action  : restart or recreate : recreate a new Vm instance from the template or, restart the same instance with the updated template
4. Update rollout Mode : Proactive  (Does the default rollout) . 
          Oppurtunistic : creates a new instance with the updated version only when the system scales up. Else only the old version is served
		  The old Vm instances are deleted when we need to scale down.
5. Vm instance can be ReCreated or ReStarted.
		  
		  
		  
Diff between MAX_UNAVAILABLE & MAX_SURGE
----------------------------------------------


		  
Rolling Update  : 
----------------------
Control how many VM updated at a time.
Mode  : ProActive.
Update only certain number of Vm at a time.
Do the health check.
Dont route the traffic to the updated instance untill they ensure running fine.

Canary Update  :
----------------------
Useful for A/B testing.
New template 30% 
Old template 70%


The ratio is fixed and maintained. The ratio is maintained even when the Vm instances scales.
So do run the new template in production and collect statistics and apply the same for rest iof the Vm when the prod results are ok.


Faster rollout without reduction in services:
Go for this approach when you dont wnat any of the runnnig instances to be unavailbale due to update.
New template to 100%
Max unavilable Vm  : 0 : So no existing Vms are updated. Instead new instances are created with the new template.
Max surge to 100%.
Create 100% extra Vm ins the new template .
Update all instances at a time. procative mode of deployment.
Remove the old Vm only when the new VM pass the health check.


How updater works with Autohealing
---------------------------------------
Say like Max unavailable is 2
tghen only 2 instances are expected to unavailable while update.
lets say one of the running Vm with the old template goes down after new temp is applied to 2 existing instaces.
So for the second rollout batch , including one of the old Vm whihc is down, then updater simply apply the new changes only to one VM, to maintain the max unavailable count whihc is 2 in this case.
If 2 Vms are auto healed no new updates are applied. Updater waits for the autohealing of theose 2 Vms to be completed.



How Updater and Autoscaling works together:
------------------------------------------------
The auo scler comes into picture when the load increases with the existing VM.
So the auoscler adds new Vm to meet the current load demand.
However when we do some update to the existing 2 VMs, the load on the other eixtsing VMs increases. 
but the auot scaler si aware that it is due to the update rllout that the load is spiking up, so it would wait for thw rollout to be completed and would not add any new instances until the rollout completes.
Sp Updater and AutoScaler wokr s well together in this case.



Updater evenly rollsout new version across the zones.Also it removes them evenly.
---------------------------------------------------------------------------------------


Updater and Load balancer:
-----------------------------
Worried about interuppting live user session?
No new tarrfic are arouted to the existing Vm which is to be updated.
The LB waits for live session to be complted and then wiats until the connection drain timeout (configured in the LB)
Then it recerates a new VM instance and send the traffic to that updated instance.


AutoScaling based on the task Queue:
--------------------------------------------
For batch we have producers and workers(like spring worker or processors)
Each worker runs in a VM instances.
the number of worker is decided dynamically by the Auo Scaler based on the number of items or task to be processed
The Auto scaler shuts down all running Vm instances when no more task is pending to be processsed.


		  


Container Optimized OS is pre-installed with Docker. So you can menton the image name in the instance template so each instance in the MIG would be started with the container running.
                 			
							
							
Unmanaged instance groups :
--------------------------------
Unmanaged instance groups can contain heterogeneous instances that you can arbitrarily add and remove from the group		
Unmanaged instance groups do not offer autoscaling, autohealing, rolling update support, multi-zone support, or the use of instance templates and are not a good fit for deploying highly available and scalable workloads.
								
 Use unmanaged instance groups if you need to apply load balancing to groups of heterogeneous instances, or if you need to manage the instances yourself.

We 


====================================================================================================================================================================================================================================================================================================================================================================================================================================================
                                                                                                        CLOUD RUN  
====================================================================================================================================================================================================================================================================================================================================================================================================================================================
Cloud Run is the serverless compute service in GCP.
It is primarily used to run cloud native Containarized apps.
So any language code can be used to run those containers in Cloud Runtime.
Cloud Code is an IDE plugin used to test locally and deploy remote of a containeraized app in cloud run. It creates a loal env with Scaffold and miniKube to test the containarized app locally.
It uses either Dockerfile or BuildPacks (used to create a container out of an app without DockerFile) to create a Container .


Cloud Run is available in two flavours : Fully Managed Cloud Run and Cloud Run on Anthos(hybrid cloud service)
It simply Auto Scales.
Pay per use.
it providea a free https endpoint. Also we can attach our service to our won custom domain.

Cloud Run has a limit of (1 hr ) in request processing limit. Beyond that U would get timeExceed exception.

